From patchwork Sat Oct 20 03:15:34 2018
Content-Type: text/plain; charset="utf-8"
MIME-Version: 1.0
Content-Transfer-Encoding: 7bit
X-Patchwork-Submitter: Junaid Shahid <junaids@google.com>
X-Patchwork-Id: 10650243
Return-Path: <kvm-owner@kernel.org>
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
 [172.30.200.125])
	by pdx-korg-patchwork-2.web.codeaurora.org (Postfix) with ESMTP id 6BFF717D4
	for <patchwork-kvm@patchwork.kernel.org>;
 Sat, 20 Oct 2018 03:16:09 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 5D58020415
	for <patchwork-kvm@patchwork.kernel.org>;
 Sat, 20 Oct 2018 03:16:09 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 516EB205F8; Sat, 20 Oct 2018 03:16:09 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-15.5 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID,DKIM_VALID_AU,MAILING_LIST_MULTI,RCVD_IN_DNSWL_HI,
	USER_IN_DEF_DKIM_WL autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id F3F8E20415
	for <patchwork-kvm@patchwork.kernel.org>;
 Sat, 20 Oct 2018 03:16:08 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726359AbeJTLY6 (ORCPT
        <rfc822;patchwork-kvm@patchwork.kernel.org>);
        Sat, 20 Oct 2018 07:24:58 -0400
Received: from mail-pg1-f201.google.com ([209.85.215.201]:47160 "EHLO
        mail-pg1-f201.google.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1725958AbeJTLY5 (ORCPT <rfc822;kvm@vger.kernel.org>);
        Sat, 20 Oct 2018 07:24:57 -0400
Received: by mail-pg1-f201.google.com with SMTP id o18-v6so25974984pgv.14
        for <kvm@vger.kernel.org>; Fri, 19 Oct 2018 20:16:05 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=google.com; s=20161025;
        h=date:in-reply-to:message-id:mime-version:references:subject:from:to
         :cc;
        bh=g+uabJgdq4xSKg/SrBUpc5Tg1R/HrgcxF2oBCcmzsvY=;
        b=vl/xIQe0v7c4MeeY5TQVogaJD9w6NsdJ2gyRa3v8MOE3/j722JxkqvvZGIMvy0mCfp
         ODCFoJ9egN/O6k+6mgJWC9GPavaDTuRds1C9I2MSMQ59IXx3EHgfuFLleeV8I9zFq95V
         7OEZlgoqGFUWYFpod7O6oONu4K/XTVDaBxd7kX7JbUybxA8NL7igtrw2I0WGtZLorUsV
         hkuJX7B31ZLJlZaw6dm1f798heUSVInnObfimBE5O6D3YMlxwgAYicKhul/m8imDD3ac
         3W+zFy6eOzfG02VCS/NSddl+Uy2rH3vJgXYpQ3p7ZdDPufQ80BrvqVUO1Q+pHd7IhA30
         v4mg==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-gm-message-state:date:in-reply-to:message-id:mime-version
         :references:subject:from:to:cc;
        bh=g+uabJgdq4xSKg/SrBUpc5Tg1R/HrgcxF2oBCcmzsvY=;
        b=MPlg7yNLOjdoPde/lka/x3r27HkvmYzXks1zgdFxIlGBhPFV//sRE6c4I69QxCnLWY
         U/rNFf275yZTeSqlH6porSlM4rPvg4c7v1VgX/voYXO5+zt4E0s7Hvy5HNRQ/8UkhcbN
         824CIU0hdYRX2Nclqls4fZBMK7b9Lk5JtQD9urAuxI/EjA0pQAmFm1LJoG2jXozcQRGM
         X4x/lHQ1kHHqdrxUYwoWR2yZKG5kNvyn01h/cxyThoZnri+YUDO3HXLwSV2XmwWbr7yw
         5+XeRoVdj3cVZntrmB3ekqyOjSXn+3+OSsaXi59iRnTg18kSnpjr+yakOyf1RcqgupDu
         5vNQ==
X-Gm-Message-State: ABuFfojS86bnOJaxLcffNxuQzjkTO7Xumhz6JtbOE7bbuM2a+vfC0Rf2
        4SI5/J440ALtjEjAfyINIi2qipzPUY29
X-Google-Smtp-Source: 
 ACcGV6016YGev7vB21ivSRTncYpZSJ4xQASbLJOvdTxSR8UDsN6FJ+TwV7QwGgc93CAwTPPHJCHwrBP66rwi
X-Received: by 2002:a62:9c8d:: with SMTP id
 u13-v6mr17678835pfk.41.1540005365472;
 Fri, 19 Oct 2018 20:16:05 -0700 (PDT)
Date: Fri, 19 Oct 2018 20:15:34 -0700
In-Reply-To: <20181020031543.124399-1-junaids@google.com>
Message-Id: <20181020031543.124399-2-junaids@google.com>
Mime-Version: 1.0
References: <20181020031543.124399-1-junaids@google.com>
X-Mailer: git-send-email 2.19.1.568.g152ad8e336-goog
Subject: [PATCH 01/10] kvm: mmu: spte_write_protect optimization
From: Junaid Shahid <junaids@google.com>
To: pbonzini@redhat.com
Cc: kvm@vger.kernel.org, pfeiner@google.com
Content-Type: text/plain; charset="UTF-8"
Sender: kvm-owner@vger.kernel.org
Precedence: bulk
List-ID: <kvm.vger.kernel.org>
X-Mailing-List: kvm@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP

This change uses a lighter-weight function instead of mmu_spte_update()
in the common case in spte_write_protect(). This helps speed up the
get_dirty_log IOCTL.

Signed-off-by: Junaid Shahid <junaids@google.com>
---
 arch/x86/kvm/mmu.c | 25 +++++++++++++++++++++----
 1 file changed, 21 insertions(+), 4 deletions(-)

diff --git a/arch/x86/kvm/mmu.c b/arch/x86/kvm/mmu.c
index 4cf43ce42959..189e21c77525 100644
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@ -1460,6 +1460,22 @@ static void drop_large_spte(struct kvm_vcpu *vcpu, u64 *sptep)
 		kvm_flush_remote_tlbs(vcpu->kvm);
 }
 
+static bool spte_test_and_clear_writable(u64 *sptep)
+{
+	u64 spte = *sptep;
+
+	if (spte & PT_WRITABLE_MASK) {
+		clear_bit(PT_WRITABLE_SHIFT, (ulong *)sptep);
+
+		if (!spte_ad_enabled(spte))
+			kvm_set_pfn_dirty(spte_to_pfn(spte));
+
+		return true;
+	}
+
+	return false;
+}
+
 /*
  * Write-protect on the specified @sptep, @pt_protect indicates whether
  * spte write-protection is caused by protecting shadow page table.
@@ -1483,11 +1499,12 @@ static bool spte_write_protect(u64 *sptep, bool pt_protect)
 
 	rmap_printk("rmap_write_protect: spte %p %llx\n", sptep, *sptep);
 
-	if (pt_protect)
-		spte &= ~SPTE_MMU_WRITEABLE;
-	spte = spte & ~PT_WRITABLE_MASK;
+	if (pt_protect) {
+		spte &= ~(PT_WRITABLE_MASK | SPTE_MMU_WRITEABLE);
+		return mmu_spte_update(sptep, spte);
+	}
 
-	return mmu_spte_update(sptep, spte);
+	return spte_test_and_clear_writable(sptep);
 }
 
 static bool __rmap_write_protect(struct kvm *kvm,

From patchwork Sat Oct 20 03:15:35 2018
Content-Type: text/plain; charset="utf-8"
MIME-Version: 1.0
Content-Transfer-Encoding: 7bit
X-Patchwork-Submitter: Junaid Shahid <junaids@google.com>
X-Patchwork-Id: 10650249
Return-Path: <kvm-owner@kernel.org>
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
 [172.30.200.125])
	by pdx-korg-patchwork-2.web.codeaurora.org (Postfix) with ESMTP id 564FB17D4
	for <patchwork-kvm@patchwork.kernel.org>;
 Sat, 20 Oct 2018 03:16:14 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 45C21201F5
	for <patchwork-kvm@patchwork.kernel.org>;
 Sat, 20 Oct 2018 03:16:14 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 3A17320415; Sat, 20 Oct 2018 03:16:14 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-15.5 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID,DKIM_VALID_AU,MAILING_LIST_MULTI,RCVD_IN_DNSWL_HI,
	USER_IN_DEF_DKIM_WL autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 6F57E201F5
	for <patchwork-kvm@patchwork.kernel.org>;
 Sat, 20 Oct 2018 03:16:13 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726470AbeJTLZA (ORCPT
        <rfc822;patchwork-kvm@patchwork.kernel.org>);
        Sat, 20 Oct 2018 07:25:00 -0400
Received: from mail-ot1-f74.google.com ([209.85.210.74]:41025 "EHLO
        mail-ot1-f74.google.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1726463AbeJTLZA (ORCPT <rfc822;kvm@vger.kernel.org>);
        Sat, 20 Oct 2018 07:25:00 -0400
Received: by mail-ot1-f74.google.com with SMTP id v40so25760300ote.8
        for <kvm@vger.kernel.org>; Fri, 19 Oct 2018 20:16:08 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=google.com; s=20161025;
        h=date:in-reply-to:message-id:mime-version:references:subject:from:to
         :cc;
        bh=aUBQZ4zsyVzfianRYLCBB/fBfPKQ4CEBLZ/TYONrLvI=;
        b=dBGkdwXOWpY1MxJ3O/WezHoPiwGOputts/RMEzpp6m5S9oeNC53DMWT0GSVWk0388T
         1Xmb5rfivAhyeNofuJHPc1kS7PvZQIt2/lz+FmB8xy88KjVRknIf/afuMWFcScyG271R
         Yd9KYiQNyw285DSvexYD3ePJkJd1QDPifcmMvJ+B4zr5BgfPnaLHaTVIoGmDNQzf1v/D
         1l1AwD0fwypdK8WczUx3/SmB2kMbmdb7xcNS+KfqekFC2frxgEcRsUM0hPVpf6CIywAk
         zBGqebOZIJHNREh9RaTlex1W7acVarNqHOtoc7VeAMp6vsNn9DL6zjvuOZK9LSUtj9wM
         OE/A==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-gm-message-state:date:in-reply-to:message-id:mime-version
         :references:subject:from:to:cc;
        bh=aUBQZ4zsyVzfianRYLCBB/fBfPKQ4CEBLZ/TYONrLvI=;
        b=ZC5kqbj92PivyMws15SMuwhY8zk7cXpj6if/ZsBuxJ9z50OnUnrq7ayUUFrnNdET/d
         TazZvkCJrVhClQ4dZoWcPettIo3/sQmD7qDDnjYekgtABJ7p/p4tS7bgSXmL3Vbma0gP
         /0amF4mqhNkWx9Srqp0uI3Q4+4Ie6Q45cAMKaLaYnz3dvxEwm6Ftv5c/NCO9Kln08UkZ
         R2jMOIIaYF3W2AWgFjPUzRnNj+FBFSscaYBmh6KksxX7OWEZMDUNOpxYAoTV5sZ3l/Gj
         dtNzvE+wkIlOxhC+WuVmTWFLMrLEMBG/9t2l3qr+8mme2OprKkddcaE8bu3orA7ORMn5
         ERUQ==
X-Gm-Message-State: ABuFfoh0F4tWZ6w8nG7c5r7ihvKDG3U0nVdH6PRDru6uH/8lwE0Yj/dP
        rKDH+2nhRz1UBliANA8tdWHQAWXZVtTR
X-Google-Smtp-Source: 
 ACcGV63I1JNyH4KbRtlKJlVpgVUcaIXWvksvt656CWkyseNQzEf0qgWXKNo0AZ6ZVMaAJumgjatys08jBw4I
X-Received: by 2002:a9d:725d:: with SMTP id a29mr28308914otk.70.1540005367763;
 Fri, 19 Oct 2018 20:16:07 -0700 (PDT)
Date: Fri, 19 Oct 2018 20:15:35 -0700
In-Reply-To: <20181020031543.124399-1-junaids@google.com>
Message-Id: <20181020031543.124399-3-junaids@google.com>
Mime-Version: 1.0
References: <20181020031543.124399-1-junaids@google.com>
X-Mailer: git-send-email 2.19.1.568.g152ad8e336-goog
Subject: [PATCH 02/10] kvm: mmu: Allow reading hardware dirty state from
 kvm_get_dirty_log_protect
From: Junaid Shahid <junaids@google.com>
To: pbonzini@redhat.com
Cc: kvm@vger.kernel.org, pfeiner@google.com
Content-Type: text/plain; charset="UTF-8"
Sender: kvm-owner@vger.kernel.org
Precedence: bulk
List-ID: <kvm.vger.kernel.org>
X-Mailing-List: kvm@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP

kvm_get_dirty_log_protect() currently gets the dirty state from the dirty
bitmap only. Any dirty state maintained by hardware has to be flushed to
the dirty bitmap beforehand. Add an ability to potentially read any
hardware dirty state directly from within kvm_get_dirty_log_protect.

Signed-off-by: Junaid Shahid <junaids@google.com>
---
 arch/mips/kvm/mmu.c             | 16 +++++++++-------
 arch/x86/include/asm/kvm_host.h | 13 ++++++++-----
 arch/x86/kvm/mmu.c              | 30 ++++++++++++++++++++----------
 arch/x86/kvm/vmx.c              |  8 ++++----
 include/linux/kvm_host.h        |  8 ++++----
 virt/kvm/arm/mmu.c              |  9 +++++----
 virt/kvm/kvm_main.c             | 21 +++++++--------------
 7 files changed, 57 insertions(+), 48 deletions(-)

diff --git a/arch/mips/kvm/mmu.c b/arch/mips/kvm/mmu.c
index d8dcdb350405..561279db5374 100644
--- a/arch/mips/kvm/mmu.c
+++ b/arch/mips/kvm/mmu.c
@@ -428,7 +428,7 @@ int kvm_mips_mkclean_gpa_pt(struct kvm *kvm, gfn_t start_gfn, gfn_t end_gfn)
 }
 
 /**
- * kvm_arch_mmu_enable_log_dirty_pt_masked() - write protect dirty pages
+ * kvm_arch_mmu_get_and_reset_log_dirty() - write protect dirty pages
  * @kvm:	The KVM pointer
  * @slot:	The memory slot associated with mask
  * @gfn_offset:	The gfn offset in memory slot
@@ -438,15 +438,17 @@ int kvm_mips_mkclean_gpa_pt(struct kvm *kvm, gfn_t start_gfn, gfn_t end_gfn)
  * Walks bits set in mask write protects the associated pte's. Caller must
  * acquire @kvm->mmu_lock.
  */
-void kvm_arch_mmu_enable_log_dirty_pt_masked(struct kvm *kvm,
+void kvm_arch_mmu_get_and_reset_log_dirty(struct kvm *kvm,
 		struct kvm_memory_slot *slot,
-		gfn_t gfn_offset, unsigned long mask)
+		gfn_t gfn_offset, unsigned long *mask)
 {
-	gfn_t base_gfn = slot->base_gfn + gfn_offset;
-	gfn_t start = base_gfn +  __ffs(mask);
-	gfn_t end = base_gfn + __fls(mask);
+	if (*mask != 0) {
+		gfn_t base_gfn = slot->base_gfn + gfn_offset;
+		gfn_t start = base_gfn +  __ffs(*mask);
+		gfn_t end = base_gfn + __fls(*mask);
 
-	kvm_mips_mkclean_gpa_pt(kvm, start, end);
+		kvm_mips_mkclean_gpa_pt(kvm, start, end);
+	}
 }
 
 /*
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 55e51ff7e421..796a44d100c1 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1123,18 +1123,21 @@ struct kvm_x86_ops {
 	 *	also called when slot is created with log dirty disabled.
 	 *  - flush_log_dirty:
 	 *	called before reporting dirty_bitmap to userspace.
-	 *  - enable_log_dirty_pt_masked:
+	 *  - get_and_reset_log_dirty:
 	 *	called when reenabling log dirty for the GFNs in the mask after
-	 *	corresponding bits are cleared in slot->dirty_bitmap.
+	 *      corresponding bits are cleared in slot->dirty_bitmap. This
+	 *      function can also add any un-flushed dirty state maintained by
+	 *      the hardware to the mask (e.g. if flush_log_dirty is not
+	 *      implemented.)
 	 */
 	void (*slot_enable_log_dirty)(struct kvm *kvm,
 				      struct kvm_memory_slot *slot);
 	void (*slot_disable_log_dirty)(struct kvm *kvm,
 				       struct kvm_memory_slot *slot);
 	void (*flush_log_dirty)(struct kvm *kvm);
-	void (*enable_log_dirty_pt_masked)(struct kvm *kvm,
-					   struct kvm_memory_slot *slot,
-					   gfn_t offset, unsigned long mask);
+	void (*get_and_reset_log_dirty)(struct kvm *kvm,
+					struct kvm_memory_slot *slot,
+					gfn_t offset, unsigned long *mask);
 	int (*write_log_dirty)(struct kvm_vcpu *vcpu);
 
 	/* pmu operations of sub-arch */
diff --git a/arch/x86/kvm/mmu.c b/arch/x86/kvm/mmu.c
index 189e21c77525..752508892b08 100644
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@ -1641,24 +1641,34 @@ void kvm_mmu_clear_dirty_pt_masked(struct kvm *kvm,
 EXPORT_SYMBOL_GPL(kvm_mmu_clear_dirty_pt_masked);
 
 /**
- * kvm_arch_mmu_enable_log_dirty_pt_masked - enable dirty logging for selected
- * PT level pages.
+ * Gets the dirty state (if any) for selected PT level pages from the hardware
+ * MMU structures and resets the hardware state to track those pages again.
  *
- * It calls kvm_mmu_write_protect_pt_masked to write protect selected pages to
- * enable dirty logging for them.
+ * mask is initially set to the contents of the slot's dirty_bitmap for the
+ * pages starting at gfn_offset. Any pages marked dirty in the hardware state
+ * should also be marked in mask before this function returns.
+ *
+ * If the hardware dirty state has already been flushed to the slot's
+ * dirty_bitmap beforehand (e.g. through kvm_x86_ops->flush_log_dirty) then this
+ * function just needs to reset the hardware structures to keep tracking the
+ * pages.
+ *
+ * If the hardware does not maintain dirty state at all, then this function
+ * just write protects the selected pages to enable software-based dirty logging
+ * for them.
  *
  * Used when we do not need to care about huge page mappings: e.g. during dirty
  * logging we do not have any such mappings.
  */
-void kvm_arch_mmu_enable_log_dirty_pt_masked(struct kvm *kvm,
+void kvm_arch_mmu_get_and_reset_log_dirty(struct kvm *kvm,
 				struct kvm_memory_slot *slot,
-				gfn_t gfn_offset, unsigned long mask)
+				gfn_t gfn_offset, unsigned long *mask)
 {
-	if (kvm_x86_ops->enable_log_dirty_pt_masked)
-		kvm_x86_ops->enable_log_dirty_pt_masked(kvm, slot, gfn_offset,
-				mask);
+	if (kvm_x86_ops->get_and_reset_log_dirty)
+		kvm_x86_ops->get_and_reset_log_dirty(kvm, slot, gfn_offset,
+						     mask);
 	else
-		kvm_mmu_write_protect_pt_masked(kvm, slot, gfn_offset, mask);
+		kvm_mmu_write_protect_pt_masked(kvm, slot, gfn_offset, *mask);
 }
 
 /**
diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c
index 641a65b30685..bb7696056072 100644
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@ -7991,7 +7991,7 @@ static __init int hardware_setup(void)
 		kvm_x86_ops->slot_enable_log_dirty = NULL;
 		kvm_x86_ops->slot_disable_log_dirty = NULL;
 		kvm_x86_ops->flush_log_dirty = NULL;
-		kvm_x86_ops->enable_log_dirty_pt_masked = NULL;
+		kvm_x86_ops->get_and_reset_log_dirty = NULL;
 	}
 
 	if (!cpu_has_vmx_preemption_timer())
@@ -14437,9 +14437,9 @@ static int vmx_write_pml_buffer(struct kvm_vcpu *vcpu)
 
 static void vmx_enable_log_dirty_pt_masked(struct kvm *kvm,
 					   struct kvm_memory_slot *memslot,
-					   gfn_t offset, unsigned long mask)
+					   gfn_t offset, unsigned long *mask)
 {
-	kvm_mmu_clear_dirty_pt_masked(kvm, memslot, offset, mask);
+	kvm_mmu_clear_dirty_pt_masked(kvm, memslot, offset, *mask);
 }
 
 static void __pi_post_block(struct kvm_vcpu *vcpu)
@@ -15076,7 +15076,7 @@ static struct kvm_x86_ops vmx_x86_ops __ro_after_init = {
 	.slot_enable_log_dirty = vmx_slot_enable_log_dirty,
 	.slot_disable_log_dirty = vmx_slot_disable_log_dirty,
 	.flush_log_dirty = vmx_flush_log_dirty,
-	.enable_log_dirty_pt_masked = vmx_enable_log_dirty_pt_masked,
+	.get_and_reset_log_dirty = vmx_enable_log_dirty_pt_masked,
 	.write_log_dirty = vmx_write_pml_buffer,
 
 	.pre_block = vmx_pre_block,
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index c926698040e0..710c3f5d50ba 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -755,10 +755,10 @@ int kvm_get_dirty_log(struct kvm *kvm,
 int kvm_get_dirty_log_protect(struct kvm *kvm,
 			struct kvm_dirty_log *log, bool *is_dirty);
 
-void kvm_arch_mmu_enable_log_dirty_pt_masked(struct kvm *kvm,
-					struct kvm_memory_slot *slot,
-					gfn_t gfn_offset,
-					unsigned long mask);
+void kvm_arch_mmu_get_and_reset_log_dirty(struct kvm *kvm,
+					  struct kvm_memory_slot *slot,
+					  gfn_t gfn_offset,
+					  unsigned long *mask);
 
 int kvm_vm_ioctl_get_dirty_log(struct kvm *kvm,
 				struct kvm_dirty_log *log);
diff --git a/virt/kvm/arm/mmu.c b/virt/kvm/arm/mmu.c
index c23a1b323aad..11a8aa7a26b4 100644
--- a/virt/kvm/arm/mmu.c
+++ b/virt/kvm/arm/mmu.c
@@ -1434,17 +1434,18 @@ static void kvm_mmu_write_protect_pt_masked(struct kvm *kvm,
 }
 
 /*
- * kvm_arch_mmu_enable_log_dirty_pt_masked - enable dirty logging for selected
+ * kvm_arch_mmu_get_and_reset_log_dirty - enable dirty logging for selected
  * dirty pages.
  *
  * It calls kvm_mmu_write_protect_pt_masked to write protect selected pages to
  * enable dirty logging for them.
  */
-void kvm_arch_mmu_enable_log_dirty_pt_masked(struct kvm *kvm,
+void kvm_arch_mmu_get_and_reset_log_dirty(struct kvm *kvm,
 		struct kvm_memory_slot *slot,
-		gfn_t gfn_offset, unsigned long mask)
+		gfn_t gfn_offset, unsigned long *mask)
 {
-	kvm_mmu_write_protect_pt_masked(kvm, slot, gfn_offset, mask);
+	if (*mask != 0)
+		kvm_mmu_write_protect_pt_masked(kvm, slot, gfn_offset, *mask);
 }
 
 static void clean_dcache_guest_page(kvm_pfn_t pfn, unsigned long size)
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 786ade1843a2..565563710687 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -1179,27 +1179,20 @@ int kvm_get_dirty_log_protect(struct kvm *kvm,
 	n = kvm_dirty_bitmap_bytes(memslot);
 
 	dirty_bitmap_buffer = kvm_second_dirty_bitmap(memslot);
-	memset(dirty_bitmap_buffer, 0, n);
 
 	spin_lock(&kvm->mmu_lock);
 	*is_dirty = false;
 	for (i = 0; i < n / sizeof(long); i++) {
-		unsigned long mask;
-		gfn_t offset;
+		unsigned long mask = 0;
+		gfn_t offset = i * BITS_PER_LONG;
 
-		if (!dirty_bitmap[i])
-			continue;
+		if (dirty_bitmap[i])
+			mask = xchg(&dirty_bitmap[i], 0);
 
-		*is_dirty = true;
-
-		mask = xchg(&dirty_bitmap[i], 0);
+		kvm_arch_mmu_get_and_reset_log_dirty(kvm, memslot, offset,
+						     &mask);
+		*is_dirty |= mask != 0;
 		dirty_bitmap_buffer[i] = mask;
-
-		if (mask) {
-			offset = i * BITS_PER_LONG;
-			kvm_arch_mmu_enable_log_dirty_pt_masked(kvm, memslot,
-								offset, mask);
-		}
 	}
 
 	spin_unlock(&kvm->mmu_lock);

From patchwork Sat Oct 20 03:15:36 2018
Content-Type: text/plain; charset="utf-8"
MIME-Version: 1.0
Content-Transfer-Encoding: 7bit
X-Patchwork-Submitter: Junaid Shahid <junaids@google.com>
X-Patchwork-Id: 10650247
Return-Path: <kvm-owner@kernel.org>
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
 [172.30.200.125])
	by pdx-korg-patchwork-2.web.codeaurora.org (Postfix) with ESMTP id 331BE13A9
	for <patchwork-kvm@patchwork.kernel.org>;
 Sat, 20 Oct 2018 03:16:14 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 22A5620415
	for <patchwork-kvm@patchwork.kernel.org>;
 Sat, 20 Oct 2018 03:16:14 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 177F1205F6; Sat, 20 Oct 2018 03:16:14 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-15.5 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID,DKIM_VALID_AU,MAILING_LIST_MULTI,RCVD_IN_DNSWL_HI,
	USER_IN_DEF_DKIM_WL autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id A4CBD20415
	for <patchwork-kvm@patchwork.kernel.org>;
 Sat, 20 Oct 2018 03:16:13 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726534AbeJTLZD (ORCPT
        <rfc822;patchwork-kvm@patchwork.kernel.org>);
        Sat, 20 Oct 2018 07:25:03 -0400
Received: from mail-qt1-f201.google.com ([209.85.160.201]:53613 "EHLO
        mail-qt1-f201.google.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1726489AbeJTLZC (ORCPT <rfc822;kvm@vger.kernel.org>);
        Sat, 20 Oct 2018 07:25:02 -0400
Received: by mail-qt1-f201.google.com with SMTP id c33-v6so40015075qta.20
        for <kvm@vger.kernel.org>; Fri, 19 Oct 2018 20:16:10 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=google.com; s=20161025;
        h=date:in-reply-to:message-id:mime-version:references:subject:from:to
         :cc;
        bh=FBsiMrI6AsfuK7l4bcXtob4sNeXsB4lggR41Yy+CnYg=;
        b=YpZrjqvPXuZMc8w7xaGAFbIFd3TublsEuShvDFpWiMobhEr0LaFj+vOTuLK8nzJGcU
         4/yIfqeoXRVqJm2XlyWkMF+YCIC7gAnpolV+94YiwwAEKtWsjkoHRKWF6K6R64DDoiY8
         FJr49EqyURSuvJWd+8cZny/QOeVi34o4ire+9cHaQuItstzCxGgB+KRzFzm+Jga1CW9L
         abMFuU+DsLpg3g/zS0On//uoZGSQvymnb66voH5/gnDiH/02jSIyVszG5oJwSrGA0Ww0
         JzOhkGP6Ki62qHEbwVnNhpHb8JpM3MNe2LY9jiFPlM2LS++s1W4fKDMNNlvauEJCc6X8
         M3xg==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-gm-message-state:date:in-reply-to:message-id:mime-version
         :references:subject:from:to:cc;
        bh=FBsiMrI6AsfuK7l4bcXtob4sNeXsB4lggR41Yy+CnYg=;
        b=rgnQ1X+embmAoUhmp69Srx6moCZIvnhmxmZNxBZDz0jQIZNZf6gli/8uh86zQvb5+C
         usiyAmzvKmGNzr40fQqWjFh0NafqkxGcb9pldyvfc8herKWY1ocRuCll2/fl18H6IBdx
         Jwt0U7ieh8z4C6dyzrFq8yKrbjlw7hY0Nw8uN1DmQCFmTpslIigq4tOVEjKLqo1uZi5S
         aMICefXENYILU/iprRkWbbgZrEdbfSXAp02Jxn0Cr7Dt/VC0iCv04Lji3Cx8lnCYjHiv
         SYM7lekjCZd0krxAKfo+cgqXWns5JdT+At7P9lP+SGwGQKyzWD2rhpgYSjeWnvLiEvtG
         yL9Q==
X-Gm-Message-State: ABuFfojVEGZ0ewO9pDxZaB2zbBzaWt3N0QqL2zRuANCKa6TmDgvE8OUZ
        +fo9A972yZFhoAiouwskPCLh1Ssp17Hf
X-Google-Smtp-Source: 
 ACcGV60Dakxu+Txo/JWZAAZfucIiLpaFGSgXaapDjh3cSd2KcVSnEAJlvk4QqsPyLn6s385BA9ps2scGbBUH
X-Received: by 2002:ac8:6a13:: with SMTP id t19-v6mr58064qtr.12.1540005370261;
 Fri, 19 Oct 2018 20:16:10 -0700 (PDT)
Date: Fri, 19 Oct 2018 20:15:36 -0700
In-Reply-To: <20181020031543.124399-1-junaids@google.com>
Message-Id: <20181020031543.124399-4-junaids@google.com>
Mime-Version: 1.0
References: <20181020031543.124399-1-junaids@google.com>
X-Mailer: git-send-email 2.19.1.568.g152ad8e336-goog
Subject: [PATCH 03/10] kvm: x86: mmu: Change __rmap_clear_dirty to
 __rmap_test_and_clear_dirty
From: Junaid Shahid <junaids@google.com>
To: pbonzini@redhat.com
Cc: kvm@vger.kernel.org, pfeiner@google.com
Content-Type: text/plain; charset="UTF-8"
Sender: kvm-owner@vger.kernel.org
Precedence: bulk
List-ID: <kvm.vger.kernel.org>
X-Mailing-List: kvm@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP

Make it explicit that the dirty bit is tested and the old value is
returned after clearing, even though in practice the function already
did that.

Signed-off-by: Junaid Shahid <junaids@google.com>
---
 arch/x86/kvm/mmu.c | 30 ++++++++++++++++++------------
 1 file changed, 18 insertions(+), 12 deletions(-)

diff --git a/arch/x86/kvm/mmu.c b/arch/x86/kvm/mmu.c
index 752508892b08..0d71a775763e 100644
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@ -1521,15 +1521,19 @@ static bool __rmap_write_protect(struct kvm *kvm,
 	return flush;
 }
 
-static bool spte_clear_dirty(u64 *sptep)
+static bool spte_test_and_clear_dirty(u64 *sptep)
 {
-	u64 spte = *sptep;
+	int dirty_bit = ffs(shadow_dirty_mask) - 1;
+	bool dirty;
 
-	rmap_printk("rmap_clear_dirty: spte %p %llx\n", sptep, *sptep);
+	BUG_ON(shadow_dirty_mask == 0);
+	rmap_printk("%s: spte %p %llx\n", __func__, sptep, *sptep);
 
-	spte &= ~shadow_dirty_mask;
+	dirty = test_and_clear_bit(dirty_bit, (unsigned long *)sptep);
+	if (dirty)
+		kvm_set_pfn_dirty(spte_to_pfn(*sptep));
 
-	return mmu_spte_update(sptep, spte);
+	return dirty;
 }
 
 static bool wrprot_ad_disabled_spte(u64 *sptep)
@@ -1548,19 +1552,20 @@ static bool wrprot_ad_disabled_spte(u64 *sptep)
  *	- W bit on ad-disabled SPTEs.
  * Returns true iff any D or W bits were cleared.
  */
-static bool __rmap_clear_dirty(struct kvm *kvm, struct kvm_rmap_head *rmap_head)
+static bool __rmap_test_and_clear_dirty(struct kvm *kvm,
+					struct kvm_rmap_head *rmap_head)
 {
 	u64 *sptep;
 	struct rmap_iterator iter;
-	bool flush = false;
+	bool dirty = false;
 
 	for_each_rmap_spte(rmap_head, &iter, sptep)
 		if (spte_ad_enabled(*sptep))
-			flush |= spte_clear_dirty(sptep);
+			dirty |= spte_test_and_clear_dirty(sptep);
 		else
-			flush |= wrprot_ad_disabled_spte(sptep);
+			dirty |= wrprot_ad_disabled_spte(sptep);
 
-	return flush;
+	return dirty;
 }
 
 static bool spte_set_dirty(u64 *sptep)
@@ -1632,7 +1637,7 @@ void kvm_mmu_clear_dirty_pt_masked(struct kvm *kvm,
 	while (mask) {
 		rmap_head = __gfn_to_rmap(slot->base_gfn + gfn_offset + __ffs(mask),
 					  PT_PAGE_TABLE_LEVEL, slot);
-		__rmap_clear_dirty(kvm, rmap_head);
+		__rmap_test_and_clear_dirty(kvm, rmap_head);
 
 		/* clear the first set bit */
 		mask &= mask - 1;
@@ -5740,7 +5745,8 @@ void kvm_mmu_slot_leaf_clear_dirty(struct kvm *kvm,
 	bool flush;
 
 	spin_lock(&kvm->mmu_lock);
-	flush = slot_handle_leaf(kvm, memslot, __rmap_clear_dirty, false);
+	flush = slot_handle_leaf(kvm, memslot, __rmap_test_and_clear_dirty,
+				 false);
 	spin_unlock(&kvm->mmu_lock);
 
 	lockdep_assert_held(&kvm->slots_lock);

From patchwork Sat Oct 20 03:15:37 2018
Content-Type: text/plain; charset="utf-8"
MIME-Version: 1.0
Content-Transfer-Encoding: 7bit
X-Patchwork-Submitter: Junaid Shahid <junaids@google.com>
X-Patchwork-Id: 10650251
Return-Path: <kvm-owner@kernel.org>
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
 [172.30.200.125])
	by pdx-korg-patchwork-2.web.codeaurora.org (Postfix) with ESMTP id 8298C13A9
	for <patchwork-kvm@patchwork.kernel.org>;
 Sat, 20 Oct 2018 03:16:16 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 7301E201F5
	for <patchwork-kvm@patchwork.kernel.org>;
 Sat, 20 Oct 2018 03:16:16 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 67873205A8; Sat, 20 Oct 2018 03:16:16 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-15.5 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID,DKIM_VALID_AU,MAILING_LIST_MULTI,RCVD_IN_DNSWL_HI,
	USER_IN_DEF_DKIM_WL autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id D714A201F5
	for <patchwork-kvm@patchwork.kernel.org>;
 Sat, 20 Oct 2018 03:16:15 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726566AbeJTLZF (ORCPT
        <rfc822;patchwork-kvm@patchwork.kernel.org>);
        Sat, 20 Oct 2018 07:25:05 -0400
Received: from mail-oi1-f201.google.com ([209.85.167.201]:55292 "EHLO
        mail-oi1-f201.google.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1726489AbeJTLZF (ORCPT <rfc822;kvm@vger.kernel.org>);
        Sat, 20 Oct 2018 07:25:05 -0400
Received: by mail-oi1-f201.google.com with SMTP id y68-v6so24473448oie.21
        for <kvm@vger.kernel.org>; Fri, 19 Oct 2018 20:16:12 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=google.com; s=20161025;
        h=date:in-reply-to:message-id:mime-version:references:subject:from:to
         :cc;
        bh=ZCR+kRvKZR626WPQIHDrJftKctTy4XEqFLzojcgbC7E=;
        b=Arpe383rXkO1KhKQQuMsM83Tu6w/crkRpmKDzAPADHs/ifSO2NXO1EIRaeBe7KyKlb
         Vg4PvaiM3e5SBVP0au20/KcL2EwPbUM9uv6Iia80yyDdKObDN43q1h1U0UdVcfqua0qx
         HsEqHJZ//vvurajgGqGby2flnKI/ceX47fsZZD+RnayDOpI5gRg5KkbVTSxdfNUq7C2e
         +q1amM2n8rwq4wnHyaRkV4HwUw2QIpmfgoggi+PN0bzqQef+DcFfdKsXm5K2Lvx5POa1
         W9dyCkPEp7Fh8he1TPX12NjrQAHRUoCHJLQhAYHugvJA9iDoV4MSozgMbO4qjOgcFYsz
         RAuw==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-gm-message-state:date:in-reply-to:message-id:mime-version
         :references:subject:from:to:cc;
        bh=ZCR+kRvKZR626WPQIHDrJftKctTy4XEqFLzojcgbC7E=;
        b=qc0QEXYpa7j4zoVPB//r2HRgkuxPGgcY7608C/MQRbY/8i6Sgtij+1OD9Etda4CwT9
         WFIwqtGXannMc/CSbgCtZ65Z5thayEJD6i/B8yb31oac+MXAdy0Jz6ZB9NfO0YGj4b+E
         6Isj5boDIa9DvYMZXt/BOCOedNIgpP1ERQihLu4DX8DRDkEgfMaVTirmTcuWCMsMpK1h
         /tTsJVQKuDn3B4lHCDBGu3ZQFRwAXIIE/6Fir7gE3W6YRkfcoVN33o282S6c0rLJyEpZ
         EydWFEoQGqEz4OtpZAsNnTZpNgsSRx9bJk50pQxRu1jnXI2trEuNwYN4572aPeuaYQqg
         hC9w==
X-Gm-Message-State: ABuFfohTh/N0I8umCPOu8artZb89pLvMDYMHbcFdXHWf4in8psOYqFqI
        QmyM1U997dYAYC7wk3sXXJ4NLOxlpcAD
X-Google-Smtp-Source: 
 ACcGV61wjJxDSut4QTkB7Cv9bHoeIrbxLAg+YaICgdO1eOxQxXCKcgoe1Obz4M5FqvgypLiNkuwDlnbEGWyn
X-Received: by 2002:a9d:3c41:: with SMTP id j1mr29072800ote.12.1540005372520;
 Fri, 19 Oct 2018 20:16:12 -0700 (PDT)
Date: Fri, 19 Oct 2018 20:15:37 -0700
In-Reply-To: <20181020031543.124399-1-junaids@google.com>
Message-Id: <20181020031543.124399-5-junaids@google.com>
Mime-Version: 1.0
References: <20181020031543.124399-1-junaids@google.com>
X-Mailer: git-send-email 2.19.1.568.g152ad8e336-goog
Subject: [PATCH 04/10] kvm: mmu: x86: Add dirty bit clearing notification hook
From: Junaid Shahid <junaids@google.com>
To: pbonzini@redhat.com
Cc: kvm@vger.kernel.org, pfeiner@google.com
Content-Type: text/plain; charset="UTF-8"
Sender: kvm-owner@vger.kernel.org
Precedence: bulk
List-ID: <kvm.vger.kernel.org>
X-Mailing-List: kvm@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP

Add a hook to notify when an SPTE's dirty bit is cleared outside of the
dirty logging path.

Signed-off-by: Junaid Shahid <junaids@google.com>
---
 arch/x86/kvm/mmu.c | 42 +++++++++++++++++++++++++-----------------
 1 file changed, 25 insertions(+), 17 deletions(-)

diff --git a/arch/x86/kvm/mmu.c b/arch/x86/kvm/mmu.c
index 0d71a775763e..3162cdd6698c 100644
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@ -701,6 +701,10 @@ static void mmu_spte_set(u64 *sptep, u64 new_spte)
 	__set_spte(sptep, new_spte);
 }
 
+static void spte_dirty_mask_cleared(struct kvm *kvm, u64 *sptep)
+{
+}
+
 /*
  * Update the SPTE (excluding the PFN), but do not track changes in its
  * accessed/dirty status.
@@ -737,7 +741,7 @@ static u64 mmu_spte_update_no_track(u64 *sptep, u64 new_spte)
  *
  * Returns true if the TLB needs to be flushed
  */
-static bool mmu_spte_update(u64 *sptep, u64 new_spte)
+static bool mmu_spte_update(struct kvm *kvm, u64 *sptep, u64 new_spte)
 {
 	bool flush = false;
 	u64 old_spte = mmu_spte_update_no_track(sptep, new_spte);
@@ -767,6 +771,7 @@ static bool mmu_spte_update(u64 *sptep, u64 new_spte)
 	if (is_dirty_spte(old_spte) && !is_dirty_spte(new_spte)) {
 		flush = true;
 		kvm_set_pfn_dirty(spte_to_pfn(old_spte));
+		spte_dirty_mask_cleared(kvm, sptep);
 	}
 
 	return flush;
@@ -778,7 +783,7 @@ static bool mmu_spte_update(u64 *sptep, u64 new_spte)
  * state bits, it is used to clear the last level sptep.
  * Returns non-zero if the PTE was previously valid.
  */
-static int mmu_spte_clear_track_bits(u64 *sptep)
+static int mmu_spte_clear_track_bits(struct kvm *kvm, u64 *sptep)
 {
 	kvm_pfn_t pfn;
 	u64 old_spte = *sptep;
@@ -803,8 +808,10 @@ static int mmu_spte_clear_track_bits(u64 *sptep)
 	if (is_accessed_spte(old_spte))
 		kvm_set_pfn_accessed(pfn);
 
-	if (is_dirty_spte(old_spte))
+	if (is_dirty_spte(old_spte)) {
 		kvm_set_pfn_dirty(pfn);
+		spte_dirty_mask_cleared(kvm, sptep);
+	}
 
 	return 1;
 }
@@ -1301,9 +1308,10 @@ static void __pte_list_remove(u64 *spte, struct kvm_rmap_head *rmap_head)
 	}
 }
 
-static void pte_list_remove(struct kvm_rmap_head *rmap_head, u64 *sptep)
+static void pte_list_remove(struct kvm *kvm, struct kvm_rmap_head *rmap_head,
+			    u64 *sptep)
 {
-	mmu_spte_clear_track_bits(sptep);
+	mmu_spte_clear_track_bits(kvm, sptep);
 	__pte_list_remove(sptep, rmap_head);
 }
 
@@ -1436,7 +1444,7 @@ static u64 *rmap_get_next(struct rmap_iterator *iter)
 
 static void drop_spte(struct kvm *kvm, u64 *sptep)
 {
-	if (mmu_spte_clear_track_bits(sptep))
+	if (mmu_spte_clear_track_bits(kvm, sptep))
 		rmap_remove(kvm, sptep);
 }
 
@@ -1489,7 +1497,7 @@ static bool spte_test_and_clear_writable(u64 *sptep)
  *
  * Return true if tlb need be flushed.
  */
-static bool spte_write_protect(u64 *sptep, bool pt_protect)
+static bool spte_write_protect(struct kvm *kvm, u64 *sptep, bool pt_protect)
 {
 	u64 spte = *sptep;
 
@@ -1501,7 +1509,7 @@ static bool spte_write_protect(u64 *sptep, bool pt_protect)
 
 	if (pt_protect) {
 		spte &= ~(PT_WRITABLE_MASK | SPTE_MMU_WRITEABLE);
-		return mmu_spte_update(sptep, spte);
+		return mmu_spte_update(kvm, sptep, spte);
 	}
 
 	return spte_test_and_clear_writable(sptep);
@@ -1516,7 +1524,7 @@ static bool __rmap_write_protect(struct kvm *kvm,
 	bool flush = false;
 
 	for_each_rmap_spte(rmap_head, &iter, sptep)
-		flush |= spte_write_protect(sptep, pt_protect);
+		flush |= spte_write_protect(kvm, sptep, pt_protect);
 
 	return flush;
 }
@@ -1568,7 +1576,7 @@ static bool __rmap_test_and_clear_dirty(struct kvm *kvm,
 	return dirty;
 }
 
-static bool spte_set_dirty(u64 *sptep)
+static bool spte_set_dirty(struct kvm *kvm, u64 *sptep)
 {
 	u64 spte = *sptep;
 
@@ -1576,7 +1584,7 @@ static bool spte_set_dirty(u64 *sptep)
 
 	spte |= shadow_dirty_mask;
 
-	return mmu_spte_update(sptep, spte);
+	return mmu_spte_update(kvm, sptep, spte);
 }
 
 static bool __rmap_set_dirty(struct kvm *kvm, struct kvm_rmap_head *rmap_head)
@@ -1587,7 +1595,7 @@ static bool __rmap_set_dirty(struct kvm *kvm, struct kvm_rmap_head *rmap_head)
 
 	for_each_rmap_spte(rmap_head, &iter, sptep)
 		if (spte_ad_enabled(*sptep))
-			flush |= spte_set_dirty(sptep);
+			flush |= spte_set_dirty(kvm, sptep);
 
 	return flush;
 }
@@ -1723,7 +1731,7 @@ static bool kvm_zap_rmapp(struct kvm *kvm, struct kvm_rmap_head *rmap_head)
 	while ((sptep = rmap_get_first(rmap_head, &iter))) {
 		rmap_printk("%s: spte %p %llx.\n", __func__, sptep, *sptep);
 
-		pte_list_remove(rmap_head, sptep);
+		pte_list_remove(kvm, rmap_head, sptep);
 		flush = true;
 	}
 
@@ -1759,7 +1767,7 @@ static int kvm_set_pte_rmapp(struct kvm *kvm, struct kvm_rmap_head *rmap_head,
 		need_flush = 1;
 
 		if (pte_write(*ptep)) {
-			pte_list_remove(rmap_head, sptep);
+			pte_list_remove(kvm, rmap_head, sptep);
 			goto restart;
 		} else {
 			new_spte = *sptep & ~PT64_BASE_ADDR_MASK;
@@ -1770,7 +1778,7 @@ static int kvm_set_pte_rmapp(struct kvm *kvm, struct kvm_rmap_head *rmap_head,
 
 			new_spte = mark_spte_for_access_track(new_spte);
 
-			mmu_spte_clear_track_bits(sptep);
+			mmu_spte_clear_track_bits(kvm, sptep);
 			mmu_spte_set(sptep, new_spte);
 		}
 	}
@@ -2969,7 +2977,7 @@ static int set_spte(struct kvm_vcpu *vcpu, u64 *sptep,
 		spte = mark_spte_for_access_track(spte);
 
 set_pte:
-	if (mmu_spte_update(sptep, spte))
+	if (mmu_spte_update(vcpu->kvm, sptep, spte))
 		ret |= SET_SPTE_NEED_REMOTE_TLB_FLUSH;
 done:
 	return ret;
@@ -5720,7 +5728,7 @@ static bool kvm_mmu_zap_collapsible_spte(struct kvm *kvm,
 		if (sp->role.direct &&
 			!kvm_is_reserved_pfn(pfn) &&
 			PageTransCompoundMap(pfn_to_page(pfn))) {
-			pte_list_remove(rmap_head, sptep);
+			pte_list_remove(kvm, rmap_head, sptep);
 			need_tlb_flush = 1;
 			goto restart;
 		}

From patchwork Sat Oct 20 03:15:38 2018
Content-Type: text/plain; charset="utf-8"
MIME-Version: 1.0
Content-Transfer-Encoding: 7bit
X-Patchwork-Submitter: Junaid Shahid <junaids@google.com>
X-Patchwork-Id: 10650253
Return-Path: <kvm-owner@kernel.org>
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
 [172.30.200.125])
	by pdx-korg-patchwork-2.web.codeaurora.org (Postfix) with ESMTP id 5BF7F13A9
	for <patchwork-kvm@patchwork.kernel.org>;
 Sat, 20 Oct 2018 03:16:18 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 4D2A1201F5
	for <patchwork-kvm@patchwork.kernel.org>;
 Sat, 20 Oct 2018 03:16:18 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 4177F205A8; Sat, 20 Oct 2018 03:16:18 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-15.5 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID,DKIM_VALID_AU,MAILING_LIST_MULTI,RCVD_IN_DNSWL_HI,
	USER_IN_DEF_DKIM_WL autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id C64AD201F5
	for <patchwork-kvm@patchwork.kernel.org>;
 Sat, 20 Oct 2018 03:16:17 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726626AbeJTLZH (ORCPT
        <rfc822;patchwork-kvm@patchwork.kernel.org>);
        Sat, 20 Oct 2018 07:25:07 -0400
Received: from mail-pf1-f202.google.com ([209.85.210.202]:34333 "EHLO
        mail-pf1-f202.google.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1726506AbeJTLZH (ORCPT <rfc822;kvm@vger.kernel.org>);
        Sat, 20 Oct 2018 07:25:07 -0400
Received: by mail-pf1-f202.google.com with SMTP id i81-v6so34018490pfj.1
        for <kvm@vger.kernel.org>; Fri, 19 Oct 2018 20:16:15 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=google.com; s=20161025;
        h=date:in-reply-to:message-id:mime-version:references:subject:from:to
         :cc;
        bh=pueMMubvqgLDxzIH+0gulG4qk/frBhiPEv9RDAgCOjc=;
        b=ZRxdydcPOzIhL1E3VZiv0bAl1L9m2/1WraBW7WIt+ImelGKAQ7DhLarOkKYgMHGBX1
         hDgCuByDjfSHlAXIhmQGFPtdC3y8bNP2DhVy+178+gtStGdvaYaH6pTyx5E8q4i798OD
         Zt9B2yc6Ktk4fDO7zNPkVqGIU+E6tpfYiKEJTdRDv42l8IexzmwBUkEic02uwrM8bgTj
         IfpLE7dpVao5DXYYGbpmLV+GV+lHARTB1QqBmsvSCjNQPHqOoY8Qd5Y8fbfEwH+2oHl7
         Y+6CUimpUBncJqjqRmN1Eh+cigy7zO+Q+uedO/mKAGzuLeHYqsuDILm/zbe7jpe+1oEG
         dStw==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-gm-message-state:date:in-reply-to:message-id:mime-version
         :references:subject:from:to:cc;
        bh=pueMMubvqgLDxzIH+0gulG4qk/frBhiPEv9RDAgCOjc=;
        b=bWxYHIZg21+TmpjVBjrDInn99LuAeNea9+jcFjm9XzJGxZmTApSsmR7zxlBtHpxamr
         UUyBH5vfvwNv1lDsOWKQIvQZ/oYr28atDeAe6VmJycG8Fa2VqSrsjA0CRS9vl+4mU4BN
         sDYb/6Lj6HL6TjKaeG4KFCAcU+V5p0BTbPVXQ1dSr28DaOdgXRPdEOuGqe8/hH5RR3gB
         AuD88hOhudGx2XdOXTsV8WVep6J4vDhVhxyDw/iABgo5zLzDCga3Fzsfc15Z7EJphq3j
         yeLWwTQ2UaJZLUA7X/1INBhuZ9Fw7qFQtnMw1PNil9pZdXHpGWwjA4xS3xvCdoksyk2D
         0jeA==
X-Gm-Message-State: ABuFfogMYvazvRth8byIfANF4caJkKxoMlnJBqRsHryAdnfl7dG7dI1l
        BmS8ixnrkebV9z79L0ilHPNEIUr0UDvZ
X-Google-Smtp-Source: 
 ACcGV62QLDJTqNrZCAmh8RZO97ddVp58MP17xJE0u1l/4NJkZYAMG2Iubsd3sILxBxxpAZNSlWC6CNm57yaT
X-Received: by 2002:a62:9c8d:: with SMTP id
 u13-v6mr17678898pfk.41.1540005374788;
 Fri, 19 Oct 2018 20:16:14 -0700 (PDT)
Date: Fri, 19 Oct 2018 20:15:38 -0700
In-Reply-To: <20181020031543.124399-1-junaids@google.com>
Message-Id: <20181020031543.124399-6-junaids@google.com>
Mime-Version: 1.0
References: <20181020031543.124399-1-junaids@google.com>
X-Mailer: git-send-email 2.19.1.568.g152ad8e336-goog
Subject: [PATCH 05/10] kvm: x86: mmu: Remove extra TLB flush from
 vmx_slot_enable_log_dirty()
From: Junaid Shahid <junaids@google.com>
To: pbonzini@redhat.com
Cc: kvm@vger.kernel.org, pfeiner@google.com
Content-Type: text/plain; charset="UTF-8"
Sender: kvm-owner@vger.kernel.org
Precedence: bulk
List-ID: <kvm.vger.kernel.org>
X-Mailing-List: kvm@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP

This can currently invoke two remote TLB flushes, one each from clearing
D bits and write protecting large pages. Instead, add a wrapper that does
both and then invokes a single remote TLB flush if needed.

Signed-off-by: Junaid Shahid <junaids@google.com>
---
 arch/x86/include/asm/kvm_host.h |  6 ++++--
 arch/x86/kvm/mmu.c              | 21 ++++++++++++++++-----
 arch/x86/kvm/vmx.c              |  3 +--
 3 files changed, 21 insertions(+), 9 deletions(-)

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 796a44d100c1..78187944494a 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1236,10 +1236,12 @@ void kvm_mmu_slot_remove_write_access(struct kvm *kvm,
 				      struct kvm_memory_slot *memslot);
 void kvm_mmu_zap_collapsible_sptes(struct kvm *kvm,
 				   const struct kvm_memory_slot *memslot);
-void kvm_mmu_slot_leaf_clear_dirty(struct kvm *kvm,
+bool kvm_mmu_slot_leaf_clear_dirty(struct kvm *kvm,
 				   struct kvm_memory_slot *memslot);
-void kvm_mmu_slot_largepage_remove_write_access(struct kvm *kvm,
+bool kvm_mmu_slot_largepage_remove_write_access(struct kvm *kvm,
 					struct kvm_memory_slot *memslot);
+void kvm_mmu_slot_wrprot_lpage_and_clear_dirty(struct kvm *kvm,
+					       struct kvm_memory_slot *memslot);
 void kvm_mmu_slot_set_dirty(struct kvm *kvm,
 			    struct kvm_memory_slot *memslot);
 void kvm_mmu_clear_dirty_pt_masked(struct kvm *kvm,
diff --git a/arch/x86/kvm/mmu.c b/arch/x86/kvm/mmu.c
index 3162cdd6698c..88d3ac0dae9e 100644
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@ -5747,7 +5747,7 @@ void kvm_mmu_zap_collapsible_sptes(struct kvm *kvm,
 	spin_unlock(&kvm->mmu_lock);
 }
 
-void kvm_mmu_slot_leaf_clear_dirty(struct kvm *kvm,
+bool kvm_mmu_slot_leaf_clear_dirty(struct kvm *kvm,
 				   struct kvm_memory_slot *memslot)
 {
 	bool flush;
@@ -5765,12 +5765,12 @@ void kvm_mmu_slot_leaf_clear_dirty(struct kvm *kvm,
 	 * out of mmu lock also guarantees no dirty pages will be lost in
 	 * dirty_bitmap.
 	 */
-	if (flush)
-		kvm_flush_remote_tlbs(kvm);
+
+	return flush;
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_slot_leaf_clear_dirty);
 
-void kvm_mmu_slot_largepage_remove_write_access(struct kvm *kvm,
+bool kvm_mmu_slot_largepage_remove_write_access(struct kvm *kvm,
 					struct kvm_memory_slot *memslot)
 {
 	bool flush;
@@ -5783,10 +5783,21 @@ void kvm_mmu_slot_largepage_remove_write_access(struct kvm *kvm,
 	/* see kvm_mmu_slot_remove_write_access */
 	lockdep_assert_held(&kvm->slots_lock);
 
+	return flush;
+}
+EXPORT_SYMBOL_GPL(kvm_mmu_slot_largepage_remove_write_access);
+
+void kvm_mmu_slot_wrprot_lpage_and_clear_dirty(struct kvm *kvm,
+					       struct kvm_memory_slot *memslot)
+{
+	bool flush;
+
+	flush = kvm_mmu_slot_leaf_clear_dirty(kvm, memslot);
+	flush |= kvm_mmu_slot_largepage_remove_write_access(kvm, memslot);
 	if (flush)
 		kvm_flush_remote_tlbs(kvm);
 }
-EXPORT_SYMBOL_GPL(kvm_mmu_slot_largepage_remove_write_access);
+EXPORT_SYMBOL_GPL(kvm_mmu_slot_wrprot_lpage_and_clear_dirty);
 
 void kvm_mmu_slot_set_dirty(struct kvm *kvm,
 			    struct kvm_memory_slot *memslot)
diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c
index bb7696056072..d9be8e631f17 100644
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@ -14380,8 +14380,7 @@ static void vmx_sched_in(struct kvm_vcpu *vcpu, int cpu)
 static void vmx_slot_enable_log_dirty(struct kvm *kvm,
 				     struct kvm_memory_slot *slot)
 {
-	kvm_mmu_slot_leaf_clear_dirty(kvm, slot);
-	kvm_mmu_slot_largepage_remove_write_access(kvm, slot);
+	kvm_mmu_slot_wrprot_lpage_and_clear_dirty(kvm, slot);
 }
 
 static void vmx_slot_disable_log_dirty(struct kvm *kvm,

From patchwork Sat Oct 20 03:15:39 2018
Content-Type: text/plain; charset="utf-8"
MIME-Version: 1.0
Content-Transfer-Encoding: 7bit
X-Patchwork-Submitter: Junaid Shahid <junaids@google.com>
X-Patchwork-Id: 10650255
Return-Path: <kvm-owner@kernel.org>
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
 [172.30.200.125])
	by pdx-korg-patchwork-2.web.codeaurora.org (Postfix) with ESMTP id D6E5E13A9
	for <patchwork-kvm@patchwork.kernel.org>;
 Sat, 20 Oct 2018 03:16:20 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id C52A4201F5
	for <patchwork-kvm@patchwork.kernel.org>;
 Sat, 20 Oct 2018 03:16:20 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id B4983205A8; Sat, 20 Oct 2018 03:16:20 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-15.5 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID,DKIM_VALID_AU,MAILING_LIST_MULTI,RCVD_IN_DNSWL_HI,
	USER_IN_DEF_DKIM_WL autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 34FDE201F5
	for <patchwork-kvm@patchwork.kernel.org>;
 Sat, 20 Oct 2018 03:16:20 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726698AbeJTLZK (ORCPT
        <rfc822;patchwork-kvm@patchwork.kernel.org>);
        Sat, 20 Oct 2018 07:25:10 -0400
Received: from mail-qk1-f201.google.com ([209.85.222.201]:44883 "EHLO
        mail-qk1-f201.google.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1726506AbeJTLZK (ORCPT <rfc822;kvm@vger.kernel.org>);
        Sat, 20 Oct 2018 07:25:10 -0400
Received: by mail-qk1-f201.google.com with SMTP id d1-v6so37915116qkb.11
        for <kvm@vger.kernel.org>; Fri, 19 Oct 2018 20:16:17 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=google.com; s=20161025;
        h=date:in-reply-to:message-id:mime-version:references:subject:from:to
         :cc;
        bh=FdzWMsCqsdkERyeBg18JKkAMQZ07IobtTgQbQgrDov8=;
        b=HXy9znOmfX2Q2Kqxcm/YN4epzrkMzrkxhup5UH0QO3K4j2V5JZK+uSAx7U1raHAdVN
         LR5xK91sfP8gz6Kkfw4zNf+64dF7do3GAiNc4SzzGOJMTUEb+AbAbmTJVPBYPgR2VTtn
         ZWnxsk5tGcykHDRaJQVIeOQyXTsEHIXBVpPIlYw5GK8mKf37tLnso7YirJpDwWuoz0IS
         fSsS5zVNFFSJHQrOOAmz+6zrQEJ+3nsNNXk6OjxQze34CsrogMl8DYrBKb2cPcBL7iCZ
         wv8ebWV/t0fcuTwMZKU08MEHQRtUOlBQH3P0WtnxrQSnCedWVVKaz7HtTfPNg4M0BEwX
         aZSg==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-gm-message-state:date:in-reply-to:message-id:mime-version
         :references:subject:from:to:cc;
        bh=FdzWMsCqsdkERyeBg18JKkAMQZ07IobtTgQbQgrDov8=;
        b=aFyT5HHxmCo1qHkuG4KyMs9vlUk8vFIkDfTnLV4LZcTAUvwxQ3wxueSs7nhEZtNMDD
         0bGjJ5BO+XMQ4PveH4LGW/bZoynci/MpyGSjp0LK4rdUlSaNR6yYn6/0VQOdtEONl8rL
         Kc3nabr25aD50echb9n+TVekJpPsqwuAz2ayxg9M6INGuGh98K1tMX2i5ns9lHmz+4ee
         wLOsL2eGK2hF3VXIQ1ZwusxF18V0IIvPtJYgdl2QSvxICYKDYy8HHDeFBA6CVExKBdOk
         3DoFadAZmjJlwhiycmanZtWw9vGjofWpqYXiyhZEg5zU3qQFjmUaT4jW4bFhJcqClOI4
         K6vQ==
X-Gm-Message-State: ABuFfohE+enJTKIZ2fbgLoxvD1Oa39pklzk72QK6n+oWiut6YLXBaEoC
        P8LTSbQZ0QFTJGRP3akwyp3hrJKEGdsu
X-Google-Smtp-Source: 
 ACcGV60fUZXh1HyETvnRU2LfOGsQJmgeJYdY+BoZuizfBuaJKixFeP1s/WN7hpQHj0v5sZUKoA8rt0sJQTNI
X-Received: by 2002:a0c:9964:: with SMTP id i33mr25157836qvd.6.1540005377170;
 Fri, 19 Oct 2018 20:16:17 -0700 (PDT)
Date: Fri, 19 Oct 2018 20:15:39 -0700
In-Reply-To: <20181020031543.124399-1-junaids@google.com>
Message-Id: <20181020031543.124399-7-junaids@google.com>
Mime-Version: 1.0
References: <20181020031543.124399-1-junaids@google.com>
X-Mailer: git-send-email 2.19.1.568.g152ad8e336-goog
Subject: [PATCH 06/10] kvm: x86: mmu: Use D bit for dirty logging
From: Junaid Shahid <junaids@google.com>
To: pbonzini@redhat.com
Cc: kvm@vger.kernel.org, pfeiner@google.com
Content-Type: text/plain; charset="UTF-8"
Sender: kvm-owner@vger.kernel.org
Precedence: bulk
List-ID: <kvm.vger.kernel.org>
X-Mailing-List: kvm@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP

Add a new dirty logging mode which uses dirty bits available in hardware,
if any, instead of write-protecting the pages.

Signed-off-by: Junaid Shahid <junaids@google.com>
---
 arch/x86/kvm/mmu.c | 49 ++++++++++++++++++++++++++++++++++++++++++++++
 arch/x86/kvm/mmu.h |  2 ++
 arch/x86/kvm/x86.c |  2 ++
 3 files changed, 53 insertions(+)

diff --git a/arch/x86/kvm/mmu.c b/arch/x86/kvm/mmu.c
index 88d3ac0dae9e..3f782d33e078 100644
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@ -49,6 +49,9 @@
 #include <asm/kvm_page_track.h>
 #include "trace.h"
 
+bool __read_mostly enable_d_bit_logging;
+module_param_named(d_bit_dirty_logging, enable_d_bit_logging, bool, 0444);
+
 /*
  * When setting this variable to true it enables Two-Dimensional-Paging
  * where the hardware walks 2 page tables:
@@ -263,6 +266,7 @@ static u64 __read_mostly shadow_nonpresent_or_rsvd_lower_gfn_mask;
 static void mmu_spte_set(u64 *sptep, u64 spte);
 static union kvm_mmu_page_role
 kvm_mmu_calc_root_page_role(struct kvm_vcpu *vcpu);
+static gfn_t kvm_mmu_page_get_gfn(struct kvm_mmu_page *sp, int index);
 
 void kvm_mmu_set_mmio_spte_mask(u64 mmio_mask, u64 mmio_value)
 {
@@ -427,6 +431,9 @@ void kvm_mmu_set_mask_ptes(u64 user_mask, u64 accessed_mask,
 	shadow_present_mask = p_mask;
 	shadow_acc_track_mask = acc_track_mask;
 	shadow_me_mask = me_mask;
+
+	if (shadow_dirty_mask == 0)
+		enable_d_bit_logging = false;
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_set_mask_ptes);
 
@@ -703,6 +710,24 @@ static void mmu_spte_set(u64 *sptep, u64 new_spte)
 
 static void spte_dirty_mask_cleared(struct kvm *kvm, u64 *sptep)
 {
+	/*
+	 * If D-bit based dirty logging is in use, then whenever the D bit in a
+	 * PTE is cleared, the page needs to be marked in the dirty bitmap.
+	 * However, when Write-Protection based dirty logging is in use, this
+	 * should not be done, as in that case, the get_dirty_log IOCTL does not
+	 * clear the D bits and hence marking pages dirty on later clearings of
+	 * the D bit would result in those pages being unnecessarily reported as
+	 * dirty again in the next round.
+	 */
+	if (enable_d_bit_logging) {
+		gfn_t gfn;
+		struct kvm_mmu_page *sp = page_header(__pa(sptep));
+
+		if (sp->role.level == PT_PAGE_TABLE_LEVEL) {
+			gfn = kvm_mmu_page_get_gfn(sp, sptep - sp->spt);
+			mark_page_dirty(kvm, gfn);
+		}
+	}
 }
 
 /*
@@ -1653,6 +1678,26 @@ void kvm_mmu_clear_dirty_pt_masked(struct kvm *kvm,
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_clear_dirty_pt_masked);
 
+unsigned long
+kvm_mmu_shadow_dirty_mask_test_and_clear(struct kvm *kvm,
+					 struct kvm_memory_slot *slot,
+					 gfn_t gfn_offset)
+{
+	unsigned long i, n;
+	unsigned long mask = 0;
+	struct kvm_rmap_head *rmap_head;
+
+	n = min_t(unsigned long, BITS_PER_LONG, slot->npages - gfn_offset);
+
+	for (i = 0; i < n; i++) {
+		rmap_head = __gfn_to_rmap(slot->base_gfn + gfn_offset + i,
+					  PT_PAGE_TABLE_LEVEL, slot);
+		if (__rmap_test_and_clear_dirty(kvm, rmap_head))
+			mask |= (1UL << i);
+	}
+	return mask;
+}
+
 /**
  * Gets the dirty state (if any) for selected PT level pages from the hardware
  * MMU structures and resets the hardware state to track those pages again.
@@ -1680,6 +1725,9 @@ void kvm_arch_mmu_get_and_reset_log_dirty(struct kvm *kvm,
 	if (kvm_x86_ops->get_and_reset_log_dirty)
 		kvm_x86_ops->get_and_reset_log_dirty(kvm, slot, gfn_offset,
 						     mask);
+	else if (enable_d_bit_logging)
+		*mask |= kvm_mmu_shadow_dirty_mask_test_and_clear(kvm, slot,
+								  gfn_offset);
 	else
 		kvm_mmu_write_protect_pt_masked(kvm, slot, gfn_offset, *mask);
 }
@@ -1775,6 +1823,7 @@ static int kvm_set_pte_rmapp(struct kvm *kvm, struct kvm_rmap_head *rmap_head,
 
 			new_spte &= ~PT_WRITABLE_MASK;
 			new_spte &= ~SPTE_HOST_WRITEABLE;
+			new_spte &= ~shadow_dirty_mask;
 
 			new_spte = mark_spte_for_access_track(new_spte);
 
diff --git a/arch/x86/kvm/mmu.h b/arch/x86/kvm/mmu.h
index c7b333147c4a..6d39802a666d 100644
--- a/arch/x86/kvm/mmu.h
+++ b/arch/x86/kvm/mmu.h
@@ -43,6 +43,8 @@
 #define PT32_ROOT_LEVEL 2
 #define PT32E_ROOT_LEVEL 3
 
+extern bool enable_d_bit_logging;
+
 static inline u64 rsvd_bits(int s, int e)
 {
 	if (e < s)
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index bdcb5babfb68..ab710956b8a2 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -9304,6 +9304,8 @@ static void kvm_mmu_slot_apply_flags(struct kvm *kvm,
 	if (new->flags & KVM_MEM_LOG_DIRTY_PAGES) {
 		if (kvm_x86_ops->slot_enable_log_dirty)
 			kvm_x86_ops->slot_enable_log_dirty(kvm, new);
+		else if (enable_d_bit_logging)
+			kvm_mmu_slot_wrprot_lpage_and_clear_dirty(kvm, new);
 		else
 			kvm_mmu_slot_remove_write_access(kvm, new);
 	} else {

From patchwork Sat Oct 20 03:15:40 2018
Content-Type: text/plain; charset="utf-8"
MIME-Version: 1.0
Content-Transfer-Encoding: 7bit
X-Patchwork-Submitter: Junaid Shahid <junaids@google.com>
X-Patchwork-Id: 10650257
Return-Path: <kvm-owner@kernel.org>
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
 [172.30.200.125])
	by pdx-korg-patchwork-2.web.codeaurora.org (Postfix) with ESMTP id CEE3313B0
	for <patchwork-kvm@patchwork.kernel.org>;
 Sat, 20 Oct 2018 03:16:22 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id BFEBA201F5
	for <patchwork-kvm@patchwork.kernel.org>;
 Sat, 20 Oct 2018 03:16:22 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id B441C205A8; Sat, 20 Oct 2018 03:16:22 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-15.5 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID,DKIM_VALID_AU,MAILING_LIST_MULTI,RCVD_IN_DNSWL_HI,
	USER_IN_DEF_DKIM_WL autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 2BCAE201F5
	for <patchwork-kvm@patchwork.kernel.org>;
 Sat, 20 Oct 2018 03:16:22 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726736AbeJTLZM (ORCPT
        <rfc822;patchwork-kvm@patchwork.kernel.org>);
        Sat, 20 Oct 2018 07:25:12 -0400
Received: from mail-yw1-f73.google.com ([209.85.161.73]:37288 "EHLO
        mail-yw1-f73.google.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1726480AbeJTLZM (ORCPT <rfc822;kvm@vger.kernel.org>);
        Sat, 20 Oct 2018 07:25:12 -0400
Received: by mail-yw1-f73.google.com with SMTP id i1-v6so23124394ywd.4
        for <kvm@vger.kernel.org>; Fri, 19 Oct 2018 20:16:19 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=google.com; s=20161025;
        h=date:in-reply-to:message-id:mime-version:references:subject:from:to
         :cc;
        bh=vYTN7+yHLu9eP3I4GIu750SAqtOROZl7p3nNLoWQBT0=;
        b=GxfI1ULgOI5bfmVboxqukMC9O91L5VjCjVyfi71IZKRGUb1Ji0142eraoINNhPGyMN
         8W6hcoktHV5CHsK1Jxqvao9Rja/7Z4CKwLcOcngDOjt5Ko5qcEajR5UBY2VS8sruomcL
         Dqo0dulyMs3a6ceMGDWEPktp2Cmm5sCWZYO8IdWcMioWKjZz+X1F/qwdnArNlkR0VJj6
         8c1YPcBH96LyEWUR+Mi3ZQC2vIuGc582BRvbVdc+0aopOkfdJQfLdmzKMEXFMEaTF7E4
         HIFIrJAC0Tg2SLftoHAV0EUEBQ5TPBYJemsNW3NePT70mcClJ+ajmCI146mITA2C8526
         LNOw==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-gm-message-state:date:in-reply-to:message-id:mime-version
         :references:subject:from:to:cc;
        bh=vYTN7+yHLu9eP3I4GIu750SAqtOROZl7p3nNLoWQBT0=;
        b=KrfAVPJcC6Mvc91kPm/AR9LchTnYKSYH3SqB9ZKps402PL1PQjIDsjirpDrYcj2LeW
         /yt+umd4VsZmlbAsslIXLwQ9CkZrygmmF6tPfEjzqiQvVxC7Y8qfLltgKuT1W5i8RY4o
         xekoSeaLOkQfLloNXsPQUVq7Ge48BTizdFv7nU6rsGPppZFfc9VjeqwZQQXySIBIwXcr
         aTkD3wEyDfhNeCG2743HLIiD75ARkYduB1QlKU+4LV2EfWJmXorBHUBtXiOrMb+iEgOD
         5eDTdiHau0cwL1dkMEeiIv3VuXKtSoFWXSu1zIc+IqE8230peCMkKx6phfYB0EubG6ij
         4/mw==
X-Gm-Message-State: ABuFfogGlJPs9a37wvh7RARQKyG6UHE3saPK5rpKEUf4uO6aiqFf/iJk
        Wv2xIYXjSD5T4r3lQ9awua88fW/aivuL
X-Google-Smtp-Source: 
 ACcGV60GgdZ39nh1lSsNNtNl4yrC6i2j5SnCukPnMGJ+6jNnJT5q662Y0EKaA8Ifp0f0Ox5Z1tnTjh0UhXiX
X-Received: by 2002:a25:824f:: with SMTP id
 d15-v6mr23129027ybn.87.1540005379443;
 Fri, 19 Oct 2018 20:16:19 -0700 (PDT)
Date: Fri, 19 Oct 2018 20:15:40 -0700
In-Reply-To: <20181020031543.124399-1-junaids@google.com>
Message-Id: <20181020031543.124399-8-junaids@google.com>
Mime-Version: 1.0
References: <20181020031543.124399-1-junaids@google.com>
X-Mailer: git-send-email 2.19.1.568.g152ad8e336-goog
Subject: [PATCH 07/10] kvm: x86: mmu: Per-VM dirty logging mode
From: Junaid Shahid <junaids@google.com>
To: pbonzini@redhat.com
Cc: kvm@vger.kernel.org, pfeiner@google.com
Content-Type: text/plain; charset="UTF-8"
Sender: kvm-owner@vger.kernel.org
Precedence: bulk
List-ID: <kvm.vger.kernel.org>
X-Mailing-List: kvm@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP

Add the ability to have different VMs use different dirty logging modes.

Signed-off-by: Junaid Shahid <junaids@google.com>
---
 arch/x86/include/asm/kvm_host.h |  4 +++
 arch/x86/include/uapi/asm/kvm.h |  4 +++
 arch/x86/kvm/mmu.c              | 30 +++++++++++++++++------
 arch/x86/kvm/vmx.c              |  3 ++-
 arch/x86/kvm/x86.c              | 43 ++++++++++++++++++++++++---------
 5 files changed, 63 insertions(+), 21 deletions(-)

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 78187944494a..3da22c92a5d6 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -920,6 +920,8 @@ struct kvm_arch {
 
 	bool guest_can_read_msr_platform_info;
 	bool exception_payload_enabled;
+
+	u8 dirty_logging_mode;
 };
 
 struct kvm_vm_stat {
@@ -1199,6 +1201,8 @@ struct kvm_arch_async_pf {
 
 extern struct kvm_x86_ops *kvm_x86_ops;
 
+extern u8 kvm_default_dirty_log_mode;
+
 #define __KVM_HAVE_ARCH_VM_ALLOC
 static inline struct kvm *kvm_arch_alloc_vm(void)
 {
diff --git a/arch/x86/include/uapi/asm/kvm.h b/arch/x86/include/uapi/asm/kvm.h
index dabfcf7c3941..2b1c442bffe6 100644
--- a/arch/x86/include/uapi/asm/kvm.h
+++ b/arch/x86/include/uapi/asm/kvm.h
@@ -420,4 +420,8 @@ struct kvm_nested_state {
 	__u8 data[0];
 };
 
+#define KVM_DIRTY_LOG_MODE_WRPROT	1
+#define KVM_DIRTY_LOG_MODE_DBIT		2
+#define KVM_DIRTY_LOG_MODE_PML		4
+
 #endif /* _ASM_X86_KVM_H */
diff --git a/arch/x86/kvm/mmu.c b/arch/x86/kvm/mmu.c
index 3f782d33e078..2714dbea2d88 100644
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@ -262,6 +262,8 @@ static const u64 shadow_nonpresent_or_rsvd_mask_len = 5;
  */
 static u64 __read_mostly shadow_nonpresent_or_rsvd_lower_gfn_mask;
 
+u8 __read_mostly kvm_default_dirty_log_mode;
+EXPORT_SYMBOL_GPL(kvm_default_dirty_log_mode);
 
 static void mmu_spte_set(u64 *sptep, u64 spte);
 static union kvm_mmu_page_role
@@ -432,8 +434,12 @@ void kvm_mmu_set_mask_ptes(u64 user_mask, u64 accessed_mask,
 	shadow_acc_track_mask = acc_track_mask;
 	shadow_me_mask = me_mask;
 
-	if (shadow_dirty_mask == 0)
+	if (shadow_dirty_mask == 0) {
 		enable_d_bit_logging = false;
+
+		if (kvm_default_dirty_log_mode == KVM_DIRTY_LOG_MODE_DBIT)
+			kvm_default_dirty_log_mode = KVM_DIRTY_LOG_MODE_WRPROT;
+	}
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_set_mask_ptes);
 
@@ -719,7 +725,7 @@ static void spte_dirty_mask_cleared(struct kvm *kvm, u64 *sptep)
 	 * the D bit would result in those pages being unnecessarily reported as
 	 * dirty again in the next round.
 	 */
-	if (enable_d_bit_logging) {
+	if (kvm->arch.dirty_logging_mode == KVM_DIRTY_LOG_MODE_DBIT) {
 		gfn_t gfn;
 		struct kvm_mmu_page *sp = page_header(__pa(sptep));
 
@@ -1722,14 +1728,19 @@ void kvm_arch_mmu_get_and_reset_log_dirty(struct kvm *kvm,
 				struct kvm_memory_slot *slot,
 				gfn_t gfn_offset, unsigned long *mask)
 {
-	if (kvm_x86_ops->get_and_reset_log_dirty)
-		kvm_x86_ops->get_and_reset_log_dirty(kvm, slot, gfn_offset,
-						     mask);
-	else if (enable_d_bit_logging)
+	switch (kvm->arch.dirty_logging_mode) {
+	case KVM_DIRTY_LOG_MODE_WRPROT:
+		kvm_mmu_write_protect_pt_masked(kvm, slot, gfn_offset, *mask);
+		break;
+	case KVM_DIRTY_LOG_MODE_DBIT:
 		*mask |= kvm_mmu_shadow_dirty_mask_test_and_clear(kvm, slot,
 								  gfn_offset);
-	else
-		kvm_mmu_write_protect_pt_masked(kvm, slot, gfn_offset, *mask);
+		break;
+	default:
+		if (kvm_x86_ops->get_and_reset_log_dirty)
+			kvm_x86_ops->get_and_reset_log_dirty(kvm, slot,
+							     gfn_offset, mask);
+	}
 }
 
 /**
@@ -6057,6 +6068,9 @@ int kvm_mmu_module_init(void)
 	BUILD_BUG_ON(sizeof(union kvm_mmu_role) != sizeof(u64));
 
 	kvm_mmu_reset_all_pte_masks();
+	kvm_default_dirty_log_mode = enable_d_bit_logging
+				     ? KVM_DIRTY_LOG_MODE_DBIT
+				     : KVM_DIRTY_LOG_MODE_WRPROT;
 
 	pte_list_desc_cache = kmem_cache_create("pte_list_desc",
 					    sizeof(struct pte_list_desc),
diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c
index d9be8e631f17..c6b0477c855e 100644
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@ -7992,7 +7992,8 @@ static __init int hardware_setup(void)
 		kvm_x86_ops->slot_disable_log_dirty = NULL;
 		kvm_x86_ops->flush_log_dirty = NULL;
 		kvm_x86_ops->get_and_reset_log_dirty = NULL;
-	}
+	} else
+		kvm_default_dirty_log_mode = KVM_DIRTY_LOG_MODE_PML;
 
 	if (!cpu_has_vmx_preemption_timer())
 		kvm_x86_ops->request_immediate_exit = __kvm_request_immediate_exit;
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index ab710956b8a2..9d22d4eeb5dc 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -4404,11 +4404,17 @@ int kvm_vm_ioctl_get_dirty_log(struct kvm *kvm, struct kvm_dirty_log *log)
 
 	mutex_lock(&kvm->slots_lock);
 
-	/*
-	 * Flush potentially hardware-cached dirty pages to dirty_bitmap.
-	 */
-	if (kvm_x86_ops->flush_log_dirty)
-		kvm_x86_ops->flush_log_dirty(kvm);
+	switch (kvm->arch.dirty_logging_mode) {
+	case KVM_DIRTY_LOG_MODE_WRPROT:
+	case KVM_DIRTY_LOG_MODE_DBIT:
+		break;
+	default:
+		/*
+		 * Flush potentially hardware-cached dirty pages to dirty_bitmap.
+		 */
+		if (kvm_x86_ops->flush_log_dirty)
+			kvm_x86_ops->flush_log_dirty(kvm);
+	}
 
 	r = kvm_get_dirty_log_protect(kvm, log, &is_dirty);
 
@@ -9020,6 +9026,8 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 
 	kvm->arch.guest_can_read_msr_platform_info = true;
 
+	kvm->arch.dirty_logging_mode = kvm_default_dirty_log_mode;
+
 	INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
 	INIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);
 
@@ -9302,15 +9310,26 @@ static void kvm_mmu_slot_apply_flags(struct kvm *kvm,
 	 * See the comments in fast_page_fault().
 	 */
 	if (new->flags & KVM_MEM_LOG_DIRTY_PAGES) {
-		if (kvm_x86_ops->slot_enable_log_dirty)
-			kvm_x86_ops->slot_enable_log_dirty(kvm, new);
-		else if (enable_d_bit_logging)
-			kvm_mmu_slot_wrprot_lpage_and_clear_dirty(kvm, new);
-		else
+		switch (kvm->arch.dirty_logging_mode) {
+		case KVM_DIRTY_LOG_MODE_WRPROT:
 			kvm_mmu_slot_remove_write_access(kvm, new);
+			break;
+		case KVM_DIRTY_LOG_MODE_DBIT:
+			kvm_mmu_slot_wrprot_lpage_and_clear_dirty(kvm, new);
+			break;
+		default:
+			if (kvm_x86_ops->slot_enable_log_dirty)
+				kvm_x86_ops->slot_enable_log_dirty(kvm, new);
+		}
 	} else {
-		if (kvm_x86_ops->slot_disable_log_dirty)
-			kvm_x86_ops->slot_disable_log_dirty(kvm, new);
+		switch (kvm->arch.dirty_logging_mode) {
+		case KVM_DIRTY_LOG_MODE_WRPROT:
+		case KVM_DIRTY_LOG_MODE_DBIT:
+			break;
+		default:
+			if (kvm_x86_ops->slot_disable_log_dirty)
+				kvm_x86_ops->slot_disable_log_dirty(kvm, new);
+		}
 	}
 }
 

From patchwork Sat Oct 20 03:15:41 2018
Content-Type: text/plain; charset="utf-8"
MIME-Version: 1.0
Content-Transfer-Encoding: 7bit
X-Patchwork-Submitter: Junaid Shahid <junaids@google.com>
X-Patchwork-Id: 10650259
Return-Path: <kvm-owner@kernel.org>
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
 [172.30.200.125])
	by pdx-korg-patchwork-2.web.codeaurora.org (Postfix) with ESMTP id 4CF5813B0
	for <patchwork-kvm@patchwork.kernel.org>;
 Sat, 20 Oct 2018 03:16:24 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 3D7F3201F5
	for <patchwork-kvm@patchwork.kernel.org>;
 Sat, 20 Oct 2018 03:16:24 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 30A2C205A8; Sat, 20 Oct 2018 03:16:24 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-15.5 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID,DKIM_VALID_AU,MAILING_LIST_MULTI,RCVD_IN_DNSWL_HI,
	USER_IN_DEF_DKIM_WL autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id C41BE201F5
	for <patchwork-kvm@patchwork.kernel.org>;
 Sat, 20 Oct 2018 03:16:23 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726903AbeJTLZO (ORCPT
        <rfc822;patchwork-kvm@patchwork.kernel.org>);
        Sat, 20 Oct 2018 07:25:14 -0400
Received: from mail-pf1-f202.google.com ([209.85.210.202]:34996 "EHLO
        mail-pf1-f202.google.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1726815AbeJTLZO (ORCPT <rfc822;kvm@vger.kernel.org>);
        Sat, 20 Oct 2018 07:25:14 -0400
Received: by mail-pf1-f202.google.com with SMTP id w64-v6so5322860pfk.2
        for <kvm@vger.kernel.org>; Fri, 19 Oct 2018 20:16:21 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=google.com; s=20161025;
        h=date:in-reply-to:message-id:mime-version:references:subject:from:to
         :cc;
        bh=MXJxesMYTiaYI5MKlVe6Vts3DyxD+UbrQJf+m7Wj0V4=;
        b=O1rWAtdoXhKRhXKf8RsZ6uOCBnj4wT9GEfwIATrVVZfwSAwKOh1TA5feCxFF8x7srp
         mgr8BL6Pc+YtDXePgymQJPpq3YcnndRWtA/Gyw1A+luMoWvsDW7SJgeAiv7x9lLC5zYq
         Ws+C0wY/L+Fj3pRbuVWdVgV6HNL4Rl2cn34es7mZd3rT18PRYUxKbOWByoseJr/zJ5ac
         5SYzu3D4FmM0jKI0mHg4u9z4noTAvSWgQSoPO1/RIV45q4FFapDyrKwhrowi++nCiTOP
         0fsq4R7tMCMX+zmsV3A4jVMcwgh1piJ29p2t/XwqMn1VvFi0KMk4uwBMZUK2Iwg9vhmI
         Q5wA==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-gm-message-state:date:in-reply-to:message-id:mime-version
         :references:subject:from:to:cc;
        bh=MXJxesMYTiaYI5MKlVe6Vts3DyxD+UbrQJf+m7Wj0V4=;
        b=EUyH7PXArOlvRaMNEARxOt9oFVTBQ/arMAaei5dsEAu5KJlJvOdyLWKxZgw81S9yc1
         IP3GXazXMoP+MouzJVhYPdfMowzbwfoRTfMsJPunq24tNMwf0r03lQRK1WsJ02tOtadK
         jd8zG712JERvq0Nuyy2XBHQC9KjZ106IOKSO6twG5eUXArQx60xTKsmYTSmm9g4Ms0QI
         d+52kC0nK+8tT2t9m+t/1CsnDB7Yhn+9Nf/nBHOwCABivHSl670EPO/iLJTwFD6VR1bu
         VgvHF3jJSAkMdDTej53D30BpUvwd0+KBPJ4lrZiQSRcHL7hx0SZOlfeeqU5cmKCS47p+
         d92g==
X-Gm-Message-State: ABuFfogcnZqEZ/SSBDv+YdQDNtr/Okv9szarhqbogz/SBl33gBXsxrzx
        JENtqUCSA1hqwhCnEi+B7wIrmdlBCZGF
X-Google-Smtp-Source: 
 ACcGV60EVLRSq/wv02ACHbUsREdCcgdNQpJGg4kpdenLHZeojSLxrx1eXwjhLPOSmkCNr81l50EUlX84bWx3
X-Received: by 2002:a63:36c1:: with SMTP id
 d184-v6mr18002307pga.73.1540005381547;
 Fri, 19 Oct 2018 20:16:21 -0700 (PDT)
Date: Fri, 19 Oct 2018 20:15:41 -0700
In-Reply-To: <20181020031543.124399-1-junaids@google.com>
Message-Id: <20181020031543.124399-9-junaids@google.com>
Mime-Version: 1.0
References: <20181020031543.124399-1-junaids@google.com>
X-Mailer: git-send-email 2.19.1.568.g152ad8e336-goog
Subject: [PATCH 08/10] kvm: x86: mmu: Refactor
 kvm_mmu_slot_*_remove_write_access
From: Junaid Shahid <junaids@google.com>
To: pbonzini@redhat.com
Cc: kvm@vger.kernel.org, pfeiner@google.com
Content-Type: text/plain; charset="UTF-8"
Sender: kvm-owner@vger.kernel.org
Precedence: bulk
List-ID: <kvm.vger.kernel.org>
X-Mailing-List: kvm@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP

Create a common helper function to implement both the _remote_write_access
and _largepage_remove_write_access variants and also introduce a new
variant _leaf_remove_write_access.

Signed-off-by: Junaid Shahid <junaids@google.com>
---
 arch/x86/kvm/mmu.c | 43 ++++++++++++++++++++++++++++---------------
 1 file changed, 28 insertions(+), 15 deletions(-)

diff --git a/arch/x86/kvm/mmu.c b/arch/x86/kvm/mmu.c
index 2714dbea2d88..0839b8cfdf66 100644
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@ -5732,14 +5732,15 @@ static bool slot_rmap_write_protect(struct kvm *kvm,
 	return __rmap_write_protect(kvm, rmap_head, false);
 }
 
-void kvm_mmu_slot_remove_write_access(struct kvm *kvm,
-				      struct kvm_memory_slot *memslot)
+static bool kvm_mmu_slot_remove_write_access_at_levels(struct kvm *kvm,
+						struct kvm_memory_slot *memslot,
+						int start_level, int end_level)
 {
 	bool flush;
 
 	spin_lock(&kvm->mmu_lock);
-	flush = slot_handle_all_level(kvm, memslot, slot_rmap_write_protect,
-				      false);
+	flush = slot_handle_level(kvm, memslot, slot_rmap_write_protect,
+				  start_level, end_level, false);
 	spin_unlock(&kvm->mmu_lock);
 
 	/*
@@ -5749,6 +5750,18 @@ void kvm_mmu_slot_remove_write_access(struct kvm *kvm,
 	 */
 	lockdep_assert_held(&kvm->slots_lock);
 
+	return flush;
+}
+
+void kvm_mmu_slot_remove_write_access(struct kvm *kvm,
+				      struct kvm_memory_slot *memslot)
+{
+	bool flush;
+
+	flush = kvm_mmu_slot_remove_write_access_at_levels(kvm, memslot,
+							   PT_PAGE_TABLE_LEVEL,
+							   PT_MAX_HUGEPAGE_LEVEL);
+
 	/*
 	 * We can flush all the TLBs out of the mmu lock without TLB
 	 * corruption since we just change the spte from writable to
@@ -5833,20 +5846,20 @@ EXPORT_SYMBOL_GPL(kvm_mmu_slot_leaf_clear_dirty);
 bool kvm_mmu_slot_largepage_remove_write_access(struct kvm *kvm,
 					struct kvm_memory_slot *memslot)
 {
-	bool flush;
-
-	spin_lock(&kvm->mmu_lock);
-	flush = slot_handle_large_level(kvm, memslot, slot_rmap_write_protect,
-					false);
-	spin_unlock(&kvm->mmu_lock);
-
-	/* see kvm_mmu_slot_remove_write_access */
-	lockdep_assert_held(&kvm->slots_lock);
-
-	return flush;
+	return kvm_mmu_slot_remove_write_access_at_levels(kvm, memslot,
+						  PT_PAGE_TABLE_LEVEL + 1,
+						  PT_MAX_HUGEPAGE_LEVEL);
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_slot_largepage_remove_write_access);
 
+static bool kvm_mmu_slot_leaf_remove_write_access(struct kvm *kvm,
+					struct kvm_memory_slot *memslot)
+{
+	return kvm_mmu_slot_remove_write_access_at_levels(kvm, memslot,
+							  PT_PAGE_TABLE_LEVEL,
+							  PT_PAGE_TABLE_LEVEL);
+}
+
 void kvm_mmu_slot_wrprot_lpage_and_clear_dirty(struct kvm *kvm,
 					       struct kvm_memory_slot *memslot)
 {

From patchwork Sat Oct 20 03:15:42 2018
Content-Type: text/plain; charset="utf-8"
MIME-Version: 1.0
Content-Transfer-Encoding: 7bit
X-Patchwork-Submitter: Junaid Shahid <junaids@google.com>
X-Patchwork-Id: 10650261
Return-Path: <kvm-owner@kernel.org>
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
 [172.30.200.125])
	by pdx-korg-patchwork-2.web.codeaurora.org (Postfix) with ESMTP id D93AD13A9
	for <patchwork-kvm@patchwork.kernel.org>;
 Sat, 20 Oct 2018 03:16:27 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id C9AE6201F5
	for <patchwork-kvm@patchwork.kernel.org>;
 Sat, 20 Oct 2018 03:16:27 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id BDF3D205F7; Sat, 20 Oct 2018 03:16:27 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-15.5 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID,DKIM_VALID_AU,MAILING_LIST_MULTI,RCVD_IN_DNSWL_HI,
	USER_IN_DEF_DKIM_WL autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 04DBD201F5
	for <patchwork-kvm@patchwork.kernel.org>;
 Sat, 20 Oct 2018 03:16:27 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726772AbeJTLZR (ORCPT
        <rfc822;patchwork-kvm@patchwork.kernel.org>);
        Sat, 20 Oct 2018 07:25:17 -0400
Received: from mail-vs1-f74.google.com ([209.85.217.74]:44132 "EHLO
        mail-vs1-f74.google.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1726764AbeJTLZR (ORCPT <rfc822;kvm@vger.kernel.org>);
        Sat, 20 Oct 2018 07:25:17 -0400
Received: by mail-vs1-f74.google.com with SMTP id g128so8662782vsd.11
        for <kvm@vger.kernel.org>; Fri, 19 Oct 2018 20:16:24 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=google.com; s=20161025;
        h=date:in-reply-to:message-id:mime-version:references:subject:from:to
         :cc:content-transfer-encoding;
        bh=kE/awBma2c8hYXcS2LZfld/B93xlrD8tszJF/cxUC/A=;
        b=rqH59bWluh7SRut8CRO8WLIH3CH0/AxlCbd0Dv2ftooq1oPy7V62po8v3iv0dQXV1T
         90kiodthQ3wNy4v5yfkdHGMQJHzwFJieLHb2yT02Apt4oTGUcmVFbD6voDvJrVdv+8hI
         1ECTqSWdDh87sWDbUdcfVWbcFFvIHl+87kHtl1AqOo7gGaEAAm79p33oh217KYD0Pymd
         PgGhTgV8XYR1KJL/49WFKUMLqie3YcjYWGFXLW1eQUijWRgeEd9oE/dGFn/qSLmE+Jze
         JrcUtqkrLJe1TTsx6noP7qr2na26h5QXjhQWKKqXMdIO23oR6OXu61GVG/tTn7sAAEuk
         jwpw==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-gm-message-state:date:in-reply-to:message-id:mime-version
         :references:subject:from:to:cc:content-transfer-encoding;
        bh=kE/awBma2c8hYXcS2LZfld/B93xlrD8tszJF/cxUC/A=;
        b=ghBGkiAFCArigWvdvz0yeWZ/p80/1Lu+YGfXETuiBSAWZQOztSRTpI14Y3A4R+XZIf
         +LZBIFEnJ9IRbrFPreZBJ23mR57pI/ZPwhzK3br3rT3WsYX65NgN5pIw0qI7WOv4D3dR
         p6zCPNsmx7WB7xIm193wB5IibwXP1f7A1rXio+rCyvOMKDpAJXRO+yO3kR+ESdpa9tAL
         Ija8Xn5aNSM2YrO8TvTCTLGheNb+ss0owJlk//WP8NOwiEgVOMEJDf3/7WqW6x+rZR71
         rLe8c7bWMK9cFceD/kCexVgsAWYB2JnaZFgd1pILEa0ule1yMdR5f/UDjnOfm22gSHZg
         ZREw==
X-Gm-Message-State: ABuFfoikPO3d8HtHuNU8k2zide8crSDp1+ZviT/exiKO3gl0gaT4JkC2
        Atlxp1342uR3BAX5aWZdYo4/NaN7ZEuK
X-Google-Smtp-Source: 
 ACcGV63/az4DPRAZ2JbzFTtDuojCTxOjj6rIVK4POg6BIPWeKuzjGUqmDwNIMKLrAthQOusXFBYh3lQYO0zS
X-Received: by 2002:a1f:85cd:: with SMTP id h196mr11430140vkd.2.1540005384233;
 Fri, 19 Oct 2018 20:16:24 -0700 (PDT)
Date: Fri, 19 Oct 2018 20:15:42 -0700
In-Reply-To: <20181020031543.124399-1-junaids@google.com>
Message-Id: <20181020031543.124399-10-junaids@google.com>
Mime-Version: 1.0
References: <20181020031543.124399-1-junaids@google.com>
X-Mailer: git-send-email 2.19.1.568.g152ad8e336-goog
Subject: [PATCH 09/10] kvm: x86: mmu: Ability to switch dirty logging mode
 dynamically
From: Junaid Shahid <junaids@google.com>
To: pbonzini@redhat.com
Cc: kvm@vger.kernel.org, pfeiner@google.com
Content-Type: text/plain; charset="UTF-8"
Sender: kvm-owner@vger.kernel.org
Precedence: bulk
List-ID: <kvm.vger.kernel.org>
X-Mailing-List: kvm@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP

Add a mechanism to switch between WrProt and D-Bit based dirty logging
while the VM is running (and possibly dirty logging is already enabled).
Switching to/from PML is not currently supported, but that can be added
later.

Signed-off-by: Junaid Shahid <junaids@google.com>
---
 arch/x86/include/asm/kvm_host.h |   4 +
 arch/x86/include/uapi/asm/kvm.h |   1 +
 arch/x86/kvm/mmu.c              | 146 +++++++++++++++++++++++++++++++-
 arch/x86/kvm/mmu.h              |   1 +
 arch/x86/kvm/vmx.c              |  10 ++-
 5 files changed, 158 insertions(+), 4 deletions(-)

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 3da22c92a5d6..bdbc87a26662 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1131,6 +1131,8 @@ struct kvm_x86_ops {
 	 *      function can also add any un-flushed dirty state maintained by
 	 *      the hardware to the mask (e.g. if flush_log_dirty is not
 	 *      implemented.)
+	 *  - switch_dirty_log_mode:
+	 *      Switch to the given dirty log mode.
 	 */
 	void (*slot_enable_log_dirty)(struct kvm *kvm,
 				      struct kvm_memory_slot *slot);
@@ -1141,6 +1143,7 @@ struct kvm_x86_ops {
 					struct kvm_memory_slot *slot,
 					gfn_t offset, unsigned long *mask);
 	int (*write_log_dirty)(struct kvm_vcpu *vcpu);
+	int (*switch_dirty_log_mode)(struct kvm *kvm, u8 mode);
 
 	/* pmu operations of sub-arch */
 	const struct kvm_pmu_ops *pmu_ops;
@@ -1202,6 +1205,7 @@ struct kvm_arch_async_pf {
 extern struct kvm_x86_ops *kvm_x86_ops;
 
 extern u8 kvm_default_dirty_log_mode;
+extern u8 kvm_supported_dirty_log_modes;
 
 #define __KVM_HAVE_ARCH_VM_ALLOC
 static inline struct kvm *kvm_arch_alloc_vm(void)
diff --git a/arch/x86/include/uapi/asm/kvm.h b/arch/x86/include/uapi/asm/kvm.h
index 2b1c442bffe6..ff2ed65be75c 100644
--- a/arch/x86/include/uapi/asm/kvm.h
+++ b/arch/x86/include/uapi/asm/kvm.h
@@ -420,6 +420,7 @@ struct kvm_nested_state {
 	__u8 data[0];
 };
 
+#define KVM_DIRTY_LOG_MODE_DEFAULT	0
 #define KVM_DIRTY_LOG_MODE_WRPROT	1
 #define KVM_DIRTY_LOG_MODE_DBIT		2
 #define KVM_DIRTY_LOG_MODE_PML		4
diff --git a/arch/x86/kvm/mmu.c b/arch/x86/kvm/mmu.c
index 0839b8cfdf66..4abc75c97593 100644
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@ -265,6 +265,9 @@ static u64 __read_mostly shadow_nonpresent_or_rsvd_lower_gfn_mask;
 u8 __read_mostly kvm_default_dirty_log_mode;
 EXPORT_SYMBOL_GPL(kvm_default_dirty_log_mode);
 
+u8 __read_mostly kvm_supported_dirty_log_modes;
+EXPORT_SYMBOL_GPL(kvm_supported_dirty_log_modes);
+
 static void mmu_spte_set(u64 *sptep, u64 spte);
 static union kvm_mmu_page_role
 kvm_mmu_calc_root_page_role(struct kvm_vcpu *vcpu);
@@ -436,6 +439,7 @@ void kvm_mmu_set_mask_ptes(u64 user_mask, u64 accessed_mask,
 
 	if (shadow_dirty_mask == 0) {
 		enable_d_bit_logging = false;
+		kvm_supported_dirty_log_modes &= ~KVM_DIRTY_LOG_MODE_DBIT;
 
 		if (kvm_default_dirty_log_mode == KVM_DIRTY_LOG_MODE_DBIT)
 			kvm_default_dirty_log_mode = KVM_DIRTY_LOG_MODE_WRPROT;
@@ -1704,6 +1708,30 @@ kvm_mmu_shadow_dirty_mask_test_and_clear(struct kvm *kvm,
 	return mask;
 }
 
+/*
+ * Test the D bit in the SPTE(s) corresponding to the GFN and return true if any
+ * SPTE has the D bit set.
+ *
+ * The MMU lock should be held before calling this function.
+ */
+bool kvm_mmu_test_shadow_dirty_mask(struct kvm *kvm,
+				    struct kvm_memory_slot *slot,
+				    gfn_t gfn_offset)
+{
+	struct kvm_rmap_head *rmap_head;
+	u64 *sptep;
+	struct rmap_iterator iter;
+	u64 pte_bits = 0;
+
+	rmap_head = __gfn_to_rmap(slot->base_gfn + gfn_offset,
+			      PT_PAGE_TABLE_LEVEL, slot);
+
+	for_each_rmap_spte(rmap_head, &iter, sptep)
+		pte_bits |= *sptep;
+
+	return pte_bits & shadow_dirty_mask;
+}
+
 /**
  * Gets the dirty state (if any) for selected PT level pages from the hardware
  * MMU structures and resets the hardware state to track those pages again.
@@ -6081,9 +6109,13 @@ int kvm_mmu_module_init(void)
 	BUILD_BUG_ON(sizeof(union kvm_mmu_role) != sizeof(u64));
 
 	kvm_mmu_reset_all_pte_masks();
-	kvm_default_dirty_log_mode = enable_d_bit_logging
-				     ? KVM_DIRTY_LOG_MODE_DBIT
-				     : KVM_DIRTY_LOG_MODE_WRPROT;
+	kvm_default_dirty_log_mode = KVM_DIRTY_LOG_MODE_WRPROT;
+	kvm_supported_dirty_log_modes = KVM_DIRTY_LOG_MODE_WRPROT;
+
+	if (enable_d_bit_logging) {
+		kvm_supported_dirty_log_modes |= KVM_DIRTY_LOG_MODE_DBIT;
+		kvm_default_dirty_log_mode = KVM_DIRTY_LOG_MODE_DBIT;
+	}
 
 	pte_list_desc_cache = kmem_cache_create("pte_list_desc",
 					    sizeof(struct pte_list_desc),
@@ -6150,3 +6182,111 @@ void kvm_mmu_module_exit(void)
 	unregister_shrinker(&mmu_shrinker);
 	mmu_audit_disable();
 }
+
+static void switch_dirty_log_mode_dbit_to_wrprot(struct kvm *kvm)
+{
+	ulong i;
+	struct kvm_memslots *slots = kvm_memslots(kvm);
+	struct kvm_memory_slot *memslot;
+	bool flush = false;
+
+	kvm_for_each_memslot(memslot, slots)
+		if (memslot->flags & KVM_MEM_LOG_DIRTY_PAGES)
+			flush |= kvm_mmu_slot_leaf_remove_write_access(kvm,
+								       memslot);
+	/*
+	 * We need to ensure that the write-protection gets propagated to all
+	 * CPUs before we transfer the dirty bits to the dirty bitmap.
+	 * Otherwise, it would be possible for some other CPU to write to a
+	 * page some time after we have gone over that page in the loop below
+	 * and then the page wouldn't get marked in the dirty bitmap.
+	 */
+	if (flush)
+		kvm_flush_remote_tlbs(kvm);
+
+	spin_lock(&kvm->mmu_lock);
+
+	kvm_for_each_memslot(memslot, slots) {
+		if (!(memslot->flags & KVM_MEM_LOG_DIRTY_PAGES))
+			continue;
+
+		for (i = 0; i < memslot->npages; i++)
+			if (!test_bit(i, memslot->dirty_bitmap) &&
+			    kvm_mmu_test_shadow_dirty_mask(kvm, memslot, i))
+				set_bit(i, memslot->dirty_bitmap);
+	}
+	spin_unlock(&kvm->mmu_lock);
+
+	kvm->arch.dirty_logging_mode = KVM_DIRTY_LOG_MODE_WRPROT;
+}
+
+static void switch_dirty_log_mode_wrprot_to_dbit(struct kvm *kvm)
+{
+	struct kvm_memslots *slots = kvm_memslots(kvm);
+	struct kvm_memory_slot *memslot;
+
+	kvm_for_each_memslot(memslot, slots)
+		if (memslot->flags & KVM_MEM_LOG_DIRTY_PAGES)
+			kvm_mmu_slot_leaf_clear_dirty(kvm, memslot);
+
+	/*
+	 * No need to initiate a TLB flush here, since any page for which we
+	 * cleared the dirty bit above would already be marked in the dirty
+	 * bitmap. It isn't until the next get_dirty_log or enable_log_dirty
+	 * that the clearing of the dirty bits needs to be propagated
+	 * everywhere.
+	 */
+
+	kvm->arch.dirty_logging_mode = KVM_DIRTY_LOG_MODE_DBIT;
+
+	/*
+	 * As an optimization, we could also remove the write-protection from
+	 * all SPTEs here, rather than incurring faults as writes happen.
+	 */
+}
+
+int kvm_mmu_switch_dirty_log_mode(struct kvm *kvm, u8 mode)
+{
+	int err = 0;
+	u8 old_mode;
+
+	if (mode == KVM_DIRTY_LOG_MODE_DEFAULT)
+		mode = kvm_default_dirty_log_mode;
+
+	if (hweight8(mode) != 1)
+		return -EINVAL;
+
+	if (!(mode & kvm_supported_dirty_log_modes)) {
+		kvm_err("Dirty logging mode %u is not supported.\n", mode);
+		return -ENOTSUPP;
+	}
+
+	kvm_debug("Switching dirty logging mode from %u to %u.\n",
+		  kvm->arch.dirty_logging_mode, mode);
+
+	mutex_lock(&kvm->slots_lock);
+
+	old_mode = kvm->arch.dirty_logging_mode;
+
+	if (mode != old_mode) {
+		if (mode == KVM_DIRTY_LOG_MODE_WRPROT &&
+		    old_mode == KVM_DIRTY_LOG_MODE_DBIT)
+			switch_dirty_log_mode_dbit_to_wrprot(kvm);
+		else if (mode == KVM_DIRTY_LOG_MODE_DBIT &&
+			 old_mode == KVM_DIRTY_LOG_MODE_WRPROT)
+			switch_dirty_log_mode_wrprot_to_dbit(kvm);
+		else if (kvm_x86_ops->switch_dirty_log_mode)
+			err = kvm_x86_ops->switch_dirty_log_mode(kvm, mode);
+		else
+			err = -ENOTSUPP;
+	}
+
+	mutex_unlock(&kvm->slots_lock);
+
+	if (err)
+		kvm_err("Trying to switch dirty logging mode from "
+			"%u to %u failed with error %d.\n",
+			old_mode, mode, err);
+
+	return err;
+}
diff --git a/arch/x86/kvm/mmu.h b/arch/x86/kvm/mmu.h
index 6d39802a666d..b27dde010ec1 100644
--- a/arch/x86/kvm/mmu.h
+++ b/arch/x86/kvm/mmu.h
@@ -213,4 +213,5 @@ void kvm_mmu_gfn_allow_lpage(struct kvm_memory_slot *slot, gfn_t gfn);
 bool kvm_mmu_slot_gfn_write_protect(struct kvm *kvm,
 				    struct kvm_memory_slot *slot, u64 gfn);
 int kvm_arch_write_log_dirty(struct kvm_vcpu *vcpu);
+int kvm_mmu_switch_dirty_log_mode(struct kvm *kvm, u8 mode);
 #endif
diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c
index c6b0477c855e..232115b84fbb 100644
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@ -7992,9 +7992,17 @@ static __init int hardware_setup(void)
 		kvm_x86_ops->slot_disable_log_dirty = NULL;
 		kvm_x86_ops->flush_log_dirty = NULL;
 		kvm_x86_ops->get_and_reset_log_dirty = NULL;
-	} else
+	} else {
 		kvm_default_dirty_log_mode = KVM_DIRTY_LOG_MODE_PML;
 
+		/*
+		 * Currently, switching between PML and other modes is not
+		 * supported, so if PML is enabled, it is the only available
+		 * mode.
+		 */
+		kvm_supported_dirty_log_modes = KVM_DIRTY_LOG_MODE_PML;
+	}
+
 	if (!cpu_has_vmx_preemption_timer())
 		kvm_x86_ops->request_immediate_exit = __kvm_request_immediate_exit;
 

From patchwork Sat Oct 20 03:15:43 2018
Content-Type: text/plain; charset="utf-8"
MIME-Version: 1.0
Content-Transfer-Encoding: 7bit
X-Patchwork-Submitter: Junaid Shahid <junaids@google.com>
X-Patchwork-Id: 10650263
Return-Path: <kvm-owner@kernel.org>
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
 [172.30.200.125])
	by pdx-korg-patchwork-2.web.codeaurora.org (Postfix) with ESMTP id CA67713B0
	for <patchwork-kvm@patchwork.kernel.org>;
 Sat, 20 Oct 2018 03:16:29 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id BC022201F5
	for <patchwork-kvm@patchwork.kernel.org>;
 Sat, 20 Oct 2018 03:16:29 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id B0842205A8; Sat, 20 Oct 2018 03:16:29 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-15.5 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID,DKIM_VALID_AU,MAILING_LIST_MULTI,RCVD_IN_DNSWL_HI,
	USER_IN_DEF_DKIM_WL autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 4C03B201F5
	for <patchwork-kvm@patchwork.kernel.org>;
 Sat, 20 Oct 2018 03:16:29 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726815AbeJTLZU (ORCPT
        <rfc822;patchwork-kvm@patchwork.kernel.org>);
        Sat, 20 Oct 2018 07:25:20 -0400
Received: from mail-qk1-f202.google.com ([209.85.222.202]:33499 "EHLO
        mail-qk1-f202.google.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1726507AbeJTLZT (ORCPT <rfc822;kvm@vger.kernel.org>);
        Sat, 20 Oct 2018 07:25:19 -0400
Received: by mail-qk1-f202.google.com with SMTP id s65-v6so12370877qki.0
        for <kvm@vger.kernel.org>; Fri, 19 Oct 2018 20:16:27 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=google.com; s=20161025;
        h=date:in-reply-to:message-id:mime-version:references:subject:from:to
         :cc;
        bh=NCmmFrmoBEl6mWpASp7zEjDZjS4KWisjYIh2Gzn5tzA=;
        b=wO7R8CFFxPXbY66fwkugfYmYTy5MB4ZAcj/ve7feSLTCz8Ys6xd24FiFWoeWJTW3dU
         IsjNZPUz+jFljMge7w5b+rOLjrhtBheSc8TEuBE0thDNk6+GERMAsboUmLrvjOdBcNsg
         DiwHv/17deQs5tQIL0YCurToYHidD4KPdMOfpSFOmzLgaO6w5QXeoQO7j3k1ao0xkOb/
         cDVpz2sz1W1VP8SkxzFP0aAGuEa9zudsXjubU0sXy1Vp/5FV39fxYbyvXCL5i0uMtXrQ
         SNM0DlZ2stqjodK+YYVH7VL798eHE7EBl7Y871KRgziolmeAx9Yf6NfyxbfzOgjBEzFB
         FRSg==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-gm-message-state:date:in-reply-to:message-id:mime-version
         :references:subject:from:to:cc;
        bh=NCmmFrmoBEl6mWpASp7zEjDZjS4KWisjYIh2Gzn5tzA=;
        b=CG6ixRLt4DkAfCPdgeJPcdZGUi5UmOSxslOKUvCwjWS9whhVHNXcPp0T8QnoqLGzoE
         pmRZaRQDbhlCziV7/cSb6yCLMuYqeBAhttonx6aNoRZQWoRWfPDCrXy5lQTafDsO3SMn
         EyXUkCmlDViPGzu5xILM2eYDRrjVkrqPEGDH73BBQNJfJGMuxtOZHivAp8uSc7hs/xOG
         1qKW/7UEj9hK1hLVp3wqJk++HjuuWTBVqIA2mBnhJ3wszO2B+20lTSwiD4qXiD3bmtcL
         xRgpsv+Pf9O+DTg1tty1ydX/hKBwrboy4YQntiirsXXW5P6cSFxh2+LMO8ZFdoTHbdvW
         e0HQ==
X-Gm-Message-State: ABuFfohKtC0wAyRaCLiwGrIzUiW+Pytl0jExOEeRfFp9+eFcJqH4XEgh
        jFbWRlZFMdUfEjlJTq9JUKCPMVO7+bod
X-Google-Smtp-Source: 
 ACcGV60oEDr3s6W0doOuU7qfMHTFnoj3esy2sqDQbUZBcY2efSl8+9meSwQPd9yhyqEb1Kqc91GlVQ6dlg/7
X-Received: by 2002:ac8:43ce:: with SMTP id
 w14-v6mr25341565qtn.26.1540005386996;
 Fri, 19 Oct 2018 20:16:26 -0700 (PDT)
Date: Fri, 19 Oct 2018 20:15:43 -0700
In-Reply-To: <20181020031543.124399-1-junaids@google.com>
Message-Id: <20181020031543.124399-11-junaids@google.com>
Mime-Version: 1.0
References: <20181020031543.124399-1-junaids@google.com>
X-Mailer: git-send-email 2.19.1.568.g152ad8e336-goog
Subject: [PATCH 10/10] kvm: x86: mmu: Add IOCTLs to allow userspace to switch
 dirty logging mode
From: Junaid Shahid <junaids@google.com>
To: pbonzini@redhat.com
Cc: kvm@vger.kernel.org, pfeiner@google.com
Content-Type: text/plain; charset="UTF-8"
Sender: kvm-owner@vger.kernel.org
Precedence: bulk
List-ID: <kvm.vger.kernel.org>
X-Mailing-List: kvm@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP

Add 3 new IOCTLs to get/set the dirty logging mode and to get a mask
representing the set of supported modes.

Signed-off-by: Junaid Shahid <junaids@google.com>
---
 Documentation/virtual/kvm/api.txt | 39 +++++++++++++++++++++++++++++++
 arch/x86/kvm/x86.c                | 13 +++++++++++
 include/uapi/linux/kvm.h          |  3 +++
 3 files changed, 55 insertions(+)

diff --git a/Documentation/virtual/kvm/api.txt b/Documentation/virtual/kvm/api.txt
index cd209f7730af..4eaf8201b9b9 100644
--- a/Documentation/virtual/kvm/api.txt
+++ b/Documentation/virtual/kvm/api.txt
@@ -3753,6 +3753,45 @@ Coalesced pio is based on coalesced mmio. There is little difference
 between coalesced mmio and pio except that coalesced pio records accesses
 to I/O ports.
 
+4.117 KVM_GET_DIRTY_LOG_MODE
+
+Capability: none
+Architectures: x86
+Type: vm ioctl
+Parameters: none
+Returns: Current dirty log mode on success, < 0 on error
+
+kvm supports 3 different mechanisms for dirty logging on x86 architectures. This
+returns the mode currently configured for this VM, which can be one of:
+
+- KVM_DIRTY_LOG_MODE_WRPROT: Uses write-protection to track dirty pages
+- KVM_DIRTY_LOG_MODE_DBIT: Uses PTE Dirty bits to track dirty pages
+- KVM_DIRTY_LOG_MODE_PML: Uses Page Modification Logging to track dirty pages
+
+4.118 KVM_GET_SUPPORTED_DIRTY_LOG_MODES
+
+Capability: none
+Architectures: x86
+Type: system ioctl
+Parameters: none
+Returns: Bitmask of dirty log modes on success, < 0 on error
+
+This returns the possible set of dirty logging modes that can be configured for
+a VM. This will be a bitmask consisting of a combination of the 3 modes listed
+above.
+
+4.119 KVM_SET_DIRTY_LOG_MODE
+
+Capability: none
+Architectures: x86
+Type: vm ioctl
+Parameters: u8
+Returns: 0 on success, < 0 on error
+
+This configures the VM's dirty logging mode according to the given parameter,
+which can be one of the 3 modes listed above, or KVM_DIRTY_LOG_MODE_DEFAULT, in
+which case kvm will choose the mode depending on enabled hardware capabilities.
+
 5. The kvm_run structure
 ------------------------
 
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 9d22d4eeb5dc..4630973b2eba 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -3164,6 +3164,11 @@ long kvm_arch_dev_ioctl(struct file *filp,
 		r = msr_io(NULL, argp, do_get_msr_feature, 1);
 		break;
 	}
+	case KVM_GET_SUPPORTED_DIRTY_LOG_MODES:
+	{
+		r = kvm_supported_dirty_log_modes;
+		break;
+	}
 	default:
 		r = -EINVAL;
 	}
@@ -4824,6 +4829,14 @@ long kvm_arch_vm_ioctl(struct file *filp,
 		r = kvm_vm_ioctl_hv_eventfd(kvm, &hvevfd);
 		break;
 	}
+	case KVM_SET_DIRTY_LOG_MODE: {
+		r = kvm_mmu_switch_dirty_log_mode(kvm, arg);
+		break;
+	}
+	case KVM_GET_DIRTY_LOG_MODE: {
+		r = kvm->arch.dirty_logging_mode;
+		break;
+	}
 	default:
 		r = -ENOTTY;
 	}
diff --git a/include/uapi/linux/kvm.h b/include/uapi/linux/kvm.h
index 2b7a652c9fa4..79289b93028a 100644
--- a/include/uapi/linux/kvm.h
+++ b/include/uapi/linux/kvm.h
@@ -1224,6 +1224,9 @@ struct kvm_vfio_spapr_tce {
 					struct kvm_userspace_memory_region)
 #define KVM_SET_TSS_ADDR          _IO(KVMIO,   0x47)
 #define KVM_SET_IDENTITY_MAP_ADDR _IOW(KVMIO,  0x48, __u64)
+#define KVM_SET_DIRTY_LOG_MODE    _IO(KVMIO,   0x49)
+#define KVM_GET_DIRTY_LOG_MODE    _IO(KVMIO,   0x4a)
+#define KVM_GET_SUPPORTED_DIRTY_LOG_MODES _IO(KVMIO, 0x4b)
 
 /* enable ucontrol for s390 */
 struct kvm_s390_ucas_mapping {
