From patchwork Thu Sep 27 00:31:26 2018
Content-Type: text/plain; charset="utf-8"
MIME-Version: 1.0
Content-Transfer-Encoding: 7bit
X-Patchwork-Submitter: Wei Yang <richard.weiyang@gmail.com>
X-Patchwork-Id: 10617063
Return-Path: <kvm-owner@kernel.org>
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
 [172.30.200.125])
	by pdx-korg-patchwork-2.web.codeaurora.org (Postfix) with ESMTP id A86A4112B
	for <patchwork-kvm@patchwork.kernel.org>;
 Thu, 27 Sep 2018 00:33:10 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 97ECA283E8
	for <patchwork-kvm@patchwork.kernel.org>;
 Thu, 27 Sep 2018 00:33:10 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 8AADD285A9; Thu, 27 Sep 2018 00:33:10 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-8.0 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID,DKIM_VALID_AU,FREEMAIL_FROM,MAILING_LIST_MULTI,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id DE0C5283E8
	for <patchwork-kvm@patchwork.kernel.org>;
 Thu, 27 Sep 2018 00:33:07 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726605AbeI0Gru (ORCPT
        <rfc822;patchwork-kvm@patchwork.kernel.org>);
        Thu, 27 Sep 2018 02:47:50 -0400
Received: from mail-pf1-f195.google.com ([209.85.210.195]:41413 "EHLO
        mail-pf1-f195.google.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1726590AbeI0Gru (ORCPT <rfc822;kvm@vger.kernel.org>);
        Thu, 27 Sep 2018 02:47:50 -0400
Received: by mail-pf1-f195.google.com with SMTP id m77-v6so525453pfi.8
        for <kvm@vger.kernel.org>; Wed, 26 Sep 2018 17:32:19 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=from:to:cc:subject:date:message-id:in-reply-to:references;
        bh=EJv7AYbpIIuHpENskWR+bM8L76CzBCKh8FTINbCltDA=;
        b=UvTxhP00GoYYc1PxqqBoUgUPNvQNVCVpALC1/9iEU0dZOmKOXloSfAOTfjphepeXYz
         iQpdeyaN502NcO2pUesXSsyDuDF/vtgrrKYDSH4PJLglRGS2s7j6AY2jthyYgIYnSwG3
         zZF0+my0TpH9kzU6xcwzgh+rgO3s6Rl28BghEbCD/pJKj29RhJOMkH/hN6hrBzBGx5CE
         8o/eCSNecyw2WHIxgT/rDaunH3ke3QnWR22LJYZRq/nOr7w1NypDkHtbjx/O6clTdO/k
         f/7ZcKZtHMHVY/byZUd0R0ARRynvxqgTOKN1NtXF87lgMdIWCrEoWekAI/cBzawgPYGQ
         49BA==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
         :references;
        bh=EJv7AYbpIIuHpENskWR+bM8L76CzBCKh8FTINbCltDA=;
        b=HLLyVgpUlzS2MXASeOpVEJ+0txL6yreXZkA+sxK6+frx/YrErnzvcDuZF7NEcPz1qs
         qsSjY/XcjaIKqHchpR6Rrd8kLfciiyrX5N4xcOYXfP1fm4MYKCuN7HtpVnbJEO9ZaXWa
         C79f49VjZC2sNKFzGEn3sYSBWmMxOwgIwouDa3/Nf2BCxmOul0rbOL4VpX9N5QkDnUTw
         EeMMDJEoeroXNk5Li9NrVbYVKUmgPV0cfRMZtVLdm7MnbuIBEypyOAFnwHHo0LBSjZDl
         JEwHlhA1aGUVhFcaPDcQJOHzSaPtu8Bk95WB0xZudwXSOj6oZRZeIRQcQNPBm9Ncah46
         kmDw==
X-Gm-Message-State: ABuFfohTHgKERWtIpfmqBuKdC9b0ypbNDQaf0HglurYOXFev9KXTgGF0
        cNGHZAdIIXJHgfJdOB5giEc=
X-Google-Smtp-Source: 
 ACcGV60HYHd3HACUghvxw66Q7XHUFqd9XK2x4Si4e0p/A9aQTClVqIYfWEN3PJFlTzX3E/Cen1xtDA==
X-Received: by 2002:a62:5882:: with SMTP id
 m124-v6mr8555205pfb.249.1538008338509;
        Wed, 26 Sep 2018 17:32:18 -0700 (PDT)
Received: from localhost ([185.92.221.13])
        by smtp.gmail.com with ESMTPSA id
 k77-v6sm392148pfg.1.2018.09.26.17.32.17
        (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Wed, 26 Sep 2018 17:32:17 -0700 (PDT)
From: Wei Yang <richard.weiyang@gmail.com>
To: pbonzini@redhat.com, rkrcmar@redhat.com,
        sean.j.christopherson@intel.com
Cc: x86@kernel.org, kvm@vger.kernel.org,
        Wei Yang <richard.weiyang@gmail.com>
Subject: [PATCH v2] KVM: x86: remove check on nr_mmu_pages in
 kvm_arch_commit_memory_region()
Date: Thu, 27 Sep 2018 08:31:26 +0800
Message-Id: <20180927003126.32359-1-richard.weiyang@gmail.com>
X-Mailer: git-send-email 2.15.1
In-Reply-To: <20180926024239.26048-1-richard.weiyang@gmail.com>
References: <20180926024239.26048-1-richard.weiyang@gmail.com>
Sender: kvm-owner@vger.kernel.org
Precedence: bulk
List-ID: <kvm.vger.kernel.org>
X-Mailing-List: kvm@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP

* nr_mmu_pages would be non-zero only if kvm->arch.n_requested_mmu_pages is
  non-zero.

* nr_mmu_pages is always non-zero, since kvm_mmu_calculate_mmu_pages()
  never return zero.

Based on these two reasons, we can merge the two *if* clause and use the
return value from kvm_mmu_calculate_mmu_pages() directly. This simplify
the code and also eliminate the possibility for reader to believe
nr_mmu_pages would be zero.

Signed-off-by: Wei Yang <richard.weiyang@gmail.com>
Reviewed-by: Sean Christopherson <sean.j.christopherson@intel.com>
---
v2: refactor changelog and rename kvm_mmu_calculate_mmu_pages() based on
Sean Christopherson comment.
---
 arch/x86/include/asm/kvm_host.h | 2 +-
 arch/x86/kvm/mmu.c              | 2 +-
 arch/x86/kvm/x86.c              | 8 ++------
 3 files changed, 4 insertions(+), 8 deletions(-)

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index c5e7bb811b1e..036dede47274 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1190,7 +1190,7 @@ void kvm_mmu_clear_dirty_pt_masked(struct kvm *kvm,
 				   gfn_t gfn_offset, unsigned long mask);
 void kvm_mmu_zap_all(struct kvm *kvm);
 void kvm_mmu_invalidate_mmio_sptes(struct kvm *kvm, struct kvm_memslots *slots);
-unsigned int kvm_mmu_calculate_mmu_pages(struct kvm *kvm);
+unsigned int kvm_mmu_calculate_default_mmu_pages(struct kvm *kvm);
 void kvm_mmu_change_mmu_pages(struct kvm *kvm, unsigned int kvm_nr_mmu_pages);
 
 int load_pdptrs(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu, unsigned long cr3);
diff --git a/arch/x86/kvm/mmu.c b/arch/x86/kvm/mmu.c
index 9fa77aa24fc7..80629f5377c3 100644
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@ -5893,7 +5893,7 @@ int kvm_mmu_module_init(void)
 /*
  * Calculate mmu pages needed for kvm.
  */
-unsigned int kvm_mmu_calculate_mmu_pages(struct kvm *kvm)
+unsigned int kvm_mmu_calculate_default_mmu_pages(struct kvm *kvm)
 {
 	unsigned int nr_mmu_pages;
 	unsigned int  nr_pages = 0;
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 8614199d733b..bed566fd4ee5 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -9131,13 +9131,9 @@ void kvm_arch_commit_memory_region(struct kvm *kvm,
 				const struct kvm_memory_slot *new,
 				enum kvm_mr_change change)
 {
-	int nr_mmu_pages = 0;
-
 	if (!kvm->arch.n_requested_mmu_pages)
-		nr_mmu_pages = kvm_mmu_calculate_mmu_pages(kvm);
-
-	if (nr_mmu_pages)
-		kvm_mmu_change_mmu_pages(kvm, nr_mmu_pages);
+		kvm_mmu_change_mmu_pages(kvm,
+				kvm_mmu_calculate_default_mmu_pages(kvm));
 
 	/*
 	 * Dirty logging tracks sptes in 4k granularity, meaning that large
