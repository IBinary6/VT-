From patchwork Fri Oct 26 07:58:59 2018
Content-Type: text/plain; charset="utf-8"
MIME-Version: 1.0
Content-Transfer-Encoding: 7bit
X-Patchwork-Submitter: Marc Orr <marcorr@google.com>
X-Patchwork-Id: 10657047
Return-Path: <kvm-owner@kernel.org>
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
 [172.30.200.125])
	by pdx-korg-patchwork-2.web.codeaurora.org (Postfix) with ESMTP id 4A8F2109C
	for <patchwork-kvm@patchwork.kernel.org>;
 Fri, 26 Oct 2018 07:59:13 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 3F1872BDB3
	for <patchwork-kvm@patchwork.kernel.org>;
 Fri, 26 Oct 2018 07:59:13 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 32A7D2BDC5; Fri, 26 Oct 2018 07:59:13 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-15.5 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID,DKIM_VALID_AU,MAILING_LIST_MULTI,RCVD_IN_DNSWL_HI,
	USER_IN_DEF_DKIM_WL autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id B9AF72BDB3
	for <patchwork-kvm@patchwork.kernel.org>;
 Fri, 26 Oct 2018 07:59:12 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726341AbeJZQfL (ORCPT
        <rfc822;patchwork-kvm@patchwork.kernel.org>);
        Fri, 26 Oct 2018 12:35:11 -0400
Received: from mail-qk1-f201.google.com ([209.85.222.201]:43346 "EHLO
        mail-qk1-f201.google.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1726065AbeJZQfL (ORCPT <rfc822;kvm@vger.kernel.org>);
        Fri, 26 Oct 2018 12:35:11 -0400
Received: by mail-qk1-f201.google.com with SMTP id n64-v6so208003qkd.10
        for <kvm@vger.kernel.org>; Fri, 26 Oct 2018 00:59:10 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=google.com; s=20161025;
        h=date:in-reply-to:message-id:mime-version:references:subject:from:to
         :cc;
        bh=EtieSRL26p9Dx8QeHqJsQHLVJsEfWHnnJP7wqrv7sEY=;
        b=osAPoFgCb9WRIyu/AvbF/ow3GxTjiCHJsmamdPM91bt4LOn2BC3my6ruK3zhefxYb6
         VJ5ZFHmSRAappXwER7GI5yj/9WqHRe4LTKELygef2fDWLbmvwaOyBhqinT+cP/JXTxaV
         RgQ5PfW2R6jWoh2fL+vQRyHJ1gWqBDl2gyotwDyNxNm8/BRM+kAgoYz8Jgft4O8q09Vs
         IAoVxvheM9Ca115VyneI6clksFWI3otITwsQmBlqZW7bzhbxkAh81rylXaEoRoDO5JXg
         fy+URw1tIrxeQU3AhYKaaPS9EOT5fcm9kcS3+zdirb6ndgSaXeKNl+vQdmwhluwgWvRV
         ygBw==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-gm-message-state:date:in-reply-to:message-id:mime-version
         :references:subject:from:to:cc;
        bh=EtieSRL26p9Dx8QeHqJsQHLVJsEfWHnnJP7wqrv7sEY=;
        b=FJRb91jkfp0JmjH7/KKmBIhcyBMFucpxQ77wIUbFzUEpQ0Mk1yo+08kB4+SvIDUQjS
         QqT+T4rzy6IUdmUrUjvqV8QADi51ZPThgqXwl0YsxLkhEQne2tcDkYDd0VgqTuExpmnA
         iZ9liyzxJ+s/d1Czl9BHQSenXudeoqrY36xbDWXQzWhmKmhBUL+q+V6Az25Qz/L9/jef
         3qTX+jGRsw5ebgGlH4R72P1JFTZfisx2TjLCWs1teMgKFFXcKykTs2jWLJ9QBUFQAqsS
         bCB4s0fpHsZ+OMkY7/9AAkq1Us1FYjbmU+cYB3mfG4wiNuinDrfzVqqox3QopihZf4Uf
         kL4A==
X-Gm-Message-State: AGRZ1gK4N2LunlliRZz5U/y4XBwaV/ZSyZtGL8Fr7CNRDmpZ3p4A+23y
        xXxEqJC6yLE8zUizljqYJW7Tg2bTOrjYf3VBx9cy1I9Z6fcSXP+aQl3C7JQOWMGwCFoqscLGD7q
        8TBYLj2a3uSqqBwWK9wCapysTZfD4p1q7vIoz6r/i7HbdF/Sy7dojRa27xdK+
X-Google-Smtp-Source: 
 AJdET5dbMdcdovLu6zu8J6pdMR05UJeXumoE8K5O6rwSBG+pALlW8z71I88thZva2wkFC6VXJCk5pLKDJKXf
X-Received: by 2002:ac8:1870:: with SMTP id
 n45-v6mr1995193qtk.46.1540540750136;
 Fri, 26 Oct 2018 00:59:10 -0700 (PDT)
Date: Fri, 26 Oct 2018 00:58:59 -0700
In-Reply-To: <20181026075900.111462-1-marcorr@google.com>
Message-Id: <20181026075900.111462-2-marcorr@google.com>
Mime-Version: 1.0
References: <20181026075900.111462-1-marcorr@google.com>
X-Mailer: git-send-email 2.19.1.568.g152ad8e336-goog
Subject: [kvm PATCH v4 1/2] kvm: vmx: refactor vmx_msrs struct for vmalloc
From: Marc Orr <marcorr@google.com>
To: kvm@vger.kernel.org, jmattson@google.com, rientjes@google.com,
        konrad.wilk@oracle.com, linux-mm@kvack.org,
        akpm@linux-foundation.org, pbonzini@redhat.com, rkrcmar@redhat.com,
        willy@infradead.org, sean.j.christopherson@intel.com
Cc: Marc Orr <marcorr@google.com>
Content-Type: text/plain; charset="UTF-8"
Sender: kvm-owner@vger.kernel.org
Precedence: bulk
List-ID: <kvm.vger.kernel.org>
X-Mailing-List: kvm@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP

Previously, the vmx_msrs struct relied being aligned within a struct
that is backed by the direct map (e.g., memory allocated with kalloc()).
Specifically, this enabled the virtual addresses associated with the
struct to be translated to physical addresses. However, we'd like to
refactor the host struct, vcpu_vmx, to be allocated with vmalloc(), so
that allocation will succeed when contiguous physical memory is scarce.

Thus, this patch refactors how vmx_msrs is declared and allocated, to
ensure that it can be mapped to the physical address space, even when
vmx_msrs resides within in a vmalloc()'d struct.

Signed-off-by: Marc Orr <marcorr@google.com>
---
 arch/x86/kvm/vmx.c | 57 ++++++++++++++++++++++++++++++++++++++++++++--
 1 file changed, 55 insertions(+), 2 deletions(-)

diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c
index abeeb45d1c33..3c0303cc101d 100644
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@ -970,8 +970,25 @@ static inline int pi_test_sn(struct pi_desc *pi_desc)
 
 struct vmx_msrs {
 	unsigned int		nr;
-	struct vmx_msr_entry	val[NR_AUTOLOAD_MSRS];
+	struct vmx_msr_entry	*val;
 };
+struct kmem_cache *vmx_msr_entry_cache;
+
+/*
+ * To prevent vmx_msr_entry array from crossing a page boundary, require:
+ * sizeof(*vmx_msrs.vmx_msr_entry.val) to be a power of two. This is guaranteed
+ * through compile-time asserts that:
+ *   - NR_AUTOLOAD_MSRS * sizeof(struct vmx_msr_entry) is a power of two
+ *   - NR_AUTOLOAD_MSRS * sizeof(struct vmx_msr_entry) <= PAGE_SIZE
+ *   - The allocation of vmx_msrs.vmx_msr_entry.val is aligned to its size.
+ */
+#define CHECK_POWER_OF_TWO(val) \
+	BUILD_BUG_ON_MSG(!((val) && !((val) & ((val) - 1))), \
+	#val " is not a power of two.")
+#define CHECK_INTRA_PAGE(val) do { \
+		CHECK_POWER_OF_TWO(val); \
+		BUILD_BUG_ON(!(val <= PAGE_SIZE)); \
+	} while (0)
 
 struct vcpu_vmx {
 	struct kvm_vcpu       vcpu;
@@ -11489,6 +11506,19 @@ static struct kvm_vcpu *vmx_create_vcpu(struct kvm *kvm, unsigned int id)
 	if (!vmx)
 		return ERR_PTR(-ENOMEM);
 
+	vmx->msr_autoload.guest.val =
+		kmem_cache_zalloc(vmx_msr_entry_cache, GFP_KERNEL);
+	if (!vmx->msr_autoload.guest.val) {
+		err = -ENOMEM;
+		goto free_vmx;
+	}
+	vmx->msr_autoload.host.val =
+		kmem_cache_zalloc(vmx_msr_entry_cache, GFP_KERNEL);
+	if (!vmx->msr_autoload.host.val) {
+		err = -ENOMEM;
+		goto free_msr_autoload_guest;
+	}
+
 	vmx->vpid = allocate_vpid();
 
 	err = kvm_vcpu_init(&vmx->vcpu, kvm, id);
@@ -11576,6 +11606,10 @@ static struct kvm_vcpu *vmx_create_vcpu(struct kvm *kvm, unsigned int id)
 	kvm_vcpu_uninit(&vmx->vcpu);
 free_vcpu:
 	free_vpid(vmx->vpid);
+	kmem_cache_free(vmx_msr_entry_cache, vmx->msr_autoload.host.val);
+free_msr_autoload_guest:
+	kmem_cache_free(vmx_msr_entry_cache, vmx->msr_autoload.guest.val);
+free_vmx:
 	kmem_cache_free(kvm_vcpu_cache, vmx);
 	return ERR_PTR(err);
 }
@@ -15153,6 +15187,10 @@ module_exit(vmx_exit);
 static int __init vmx_init(void)
 {
 	int r;
+	size_t vmx_msr_entry_size =
+		sizeof(struct vmx_msr_entry) * NR_AUTOLOAD_MSRS;
+
+	CHECK_INTRA_PAGE(vmx_msr_entry_size);
 
 #if IS_ENABLED(CONFIG_HYPERV)
 	/*
@@ -15184,9 +15222,21 @@ static int __init vmx_init(void)
 #endif
 
 	r = kvm_init(&vmx_x86_ops, sizeof(struct vcpu_vmx),
-		     __alignof__(struct vcpu_vmx), THIS_MODULE);
+		__alignof__(struct vcpu_vmx), THIS_MODULE);
 	if (r)
 		return r;
+	/*
+	 * A vmx_msr_entry array resides exclusively within the kernel. Thus,
+	 * use kmem_cache_create_usercopy(), with the usersize argument set to
+	 * ZERO, to blacklist copying vmx_msr_entry to/from user space.
+	 */
+	vmx_msr_entry_cache =
+		kmem_cache_create_usercopy("vmx_msr_entry", vmx_msr_entry_size,
+				  vmx_msr_entry_size, SLAB_ACCOUNT, 0, 0, NULL);
+	if (!vmx_msr_entry_cache) {
+		r = -ENOMEM;
+		goto out;
+	}
 
 	/*
 	 * Must be called after kvm_init() so enable_ept is properly set
@@ -15210,5 +15260,8 @@ static int __init vmx_init(void)
 	vmx_check_vmcs12_offsets();
 
 	return 0;
+out:
+	kvm_exit();
+	return r;
 }
 module_init(vmx_init);

From patchwork Fri Oct 26 07:59:00 2018
Content-Type: text/plain; charset="utf-8"
MIME-Version: 1.0
Content-Transfer-Encoding: 7bit
X-Patchwork-Submitter: Marc Orr <marcorr@google.com>
X-Patchwork-Id: 10657053
Return-Path: <kvm-owner@kernel.org>
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
 [172.30.200.125])
	by pdx-korg-patchwork-2.web.codeaurora.org (Postfix) with ESMTP id 9168214E2
	for <patchwork-kvm@patchwork.kernel.org>;
 Fri, 26 Oct 2018 07:59:19 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 86F272BD78
	for <patchwork-kvm@patchwork.kernel.org>;
 Fri, 26 Oct 2018 07:59:19 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 7BAD22BDB3; Fri, 26 Oct 2018 07:59:19 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-15.5 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID,DKIM_VALID_AU,MAILING_LIST_MULTI,RCVD_IN_DNSWL_HI,
	USER_IN_DEF_DKIM_WL autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id F0FCB2BDB9
	for <patchwork-kvm@patchwork.kernel.org>;
 Fri, 26 Oct 2018 07:59:18 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726531AbeJZQfQ (ORCPT
        <rfc822;patchwork-kvm@patchwork.kernel.org>);
        Fri, 26 Oct 2018 12:35:16 -0400
Received: from mail-pf1-f201.google.com ([209.85.210.201]:37457 "EHLO
        mail-pf1-f201.google.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1726101AbeJZQfQ (ORCPT <rfc822;kvm@vger.kernel.org>);
        Fri, 26 Oct 2018 12:35:16 -0400
Received: by mail-pf1-f201.google.com with SMTP id c28-v6so184422pfe.4
        for <kvm@vger.kernel.org>; Fri, 26 Oct 2018 00:59:15 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=google.com; s=20161025;
        h=date:in-reply-to:message-id:mime-version:references:subject:from:to
         :cc;
        bh=37hMVlXqW6DIbVYja1xxGFc0YV3pHTaQ3q8Kf+MbFeM=;
        b=Kg1mM6jqDfF6ElHilPnWbAyXD9bC2H3Jea5p39scqd4zjw0/a123tuHbYLJoIhSCe0
         NumBeTRVBlVxTIhv1EJqmwiXeJKlXaPJmlNHsrl+vwXTr3LYUMrXLEV6hityGRs8LLF3
         EqH7dyFqzCDrpAy27tqIWRw8qPVWEccyciJZiL3bfFkpVNtz95ZIuG6fZF9X8p0R4pLi
         lCMkDW0gLCaNVwTGt+rRRKmbUROHChqifNGAVu+eM8Gt+NEK/1GahTSxhJ1dOcT5edkv
         Pt8mUGjFVOY66bU3uL9QLdwuVTiwyYEdbR7V6AUfENDNCTWe8fldgq8oG9SbIZFurtL+
         uusg==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-gm-message-state:date:in-reply-to:message-id:mime-version
         :references:subject:from:to:cc;
        bh=37hMVlXqW6DIbVYja1xxGFc0YV3pHTaQ3q8Kf+MbFeM=;
        b=tzieuts2TWmes6UxcvoJd5kF1YndGBUSXs64pRu52+7KeFsQp2socYKm9kj4x0w7V6
         GPbAP7CFEPReOhbnSenRkUFhmnwW3OLQ52Hct2L4dPGjh8yiAMRQyEL4I5VnJlqA52Ew
         hcNPyQjfwIW5/XbE8gJQj1pmWgtpV4ceRZ01cN2IqN3ZcAQDUtFKgi5BgcIQ4Q/zmWOK
         HZ6NzOOAIL0Q87P64fIuUomDIt5CGSLK5fPRxZcPAX2uzno1HugrPkNYbyQ1KLV5v+rn
         FIJu6TOVkNPRYmGHb5M1hAcJ2VEI//ucoPJEgCi/DkJ/cpQgkiJ/86eRLra0KEsgUHGf
         1u7g==
X-Gm-Message-State: AGRZ1gLdtI4T1CfNYK6NRKfwjbsXJpz4+oFSyowJly9gfdUekU6UfInH
        gw1DcRlTex5zZFi5O/uKo01QT02IbTFg5c6Hh93iZnpPMYRyQfa38rzqXM/TR5ADa6sUyyccAEG
        RygF+tJPO0zs9cEpWIUiQr2ksK4Mllmp1N8RS9liluptA3MnLnIqlx2fWCXef
X-Google-Smtp-Source: 
 AJdET5c4V1uisTHbPbBisk1jNojGJzukYUXU377FQqmUUgOYdAsJl3hVDzRw8+E9Peju5sSIG8lvwbqMNH4F
X-Received: by 2002:a63:c40e:: with SMTP id
 h14-v6mr1295377pgd.53.1540540754989;
 Fri, 26 Oct 2018 00:59:14 -0700 (PDT)
Date: Fri, 26 Oct 2018 00:59:00 -0700
In-Reply-To: <20181026075900.111462-1-marcorr@google.com>
Message-Id: <20181026075900.111462-3-marcorr@google.com>
Mime-Version: 1.0
References: <20181026075900.111462-1-marcorr@google.com>
X-Mailer: git-send-email 2.19.1.568.g152ad8e336-goog
Subject: [kvm PATCH v4 2/2] kvm: vmx: use vmalloc() to allocate vcpus
From: Marc Orr <marcorr@google.com>
To: kvm@vger.kernel.org, jmattson@google.com, rientjes@google.com,
        konrad.wilk@oracle.com, linux-mm@kvack.org,
        akpm@linux-foundation.org, pbonzini@redhat.com, rkrcmar@redhat.com,
        willy@infradead.org, sean.j.christopherson@intel.com
Cc: Marc Orr <marcorr@google.com>
Content-Type: text/plain; charset="UTF-8"
Sender: kvm-owner@vger.kernel.org
Precedence: bulk
List-ID: <kvm.vger.kernel.org>
X-Mailing-List: kvm@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP

Previously, vcpus were allocated through the kmem_cache_zalloc() API,
which requires the underlying physical memory to be contiguous.
Because the x86 vcpu struct, struct vcpu_vmx, is relatively large
(e.g., currently 47680 bytes on my setup), it can become hard to find
contiguous memory.

At the same time, the comments in the code indicate that the primary
reason for using the kmem_cache_zalloc() API is to align the memory
rather than to provide physical contiguity.

Thus, this patch updates the vcpu allocation logic for vmx to use the
vmalloc() API.

Signed-off-by: Marc Orr <marcorr@google.com>
---
 arch/x86/kvm/vmx.c  | 37 ++++++++++++++++++++++++++++++-------
 virt/kvm/kvm_main.c | 28 ++++++++++++++++------------
 2 files changed, 46 insertions(+), 19 deletions(-)

diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c
index 3c0303cc101d..8eef21656f60 100644
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@ -898,7 +898,14 @@ struct nested_vmx {
 #define POSTED_INTR_ON  0
 #define POSTED_INTR_SN  1
 
-/* Posted-Interrupt Descriptor */
+/*
+ * Posted-Interrupt Descriptor
+ *
+ * Note, the physical address of this structure is used by VMX. Furthermore, the
+ * translation code assumes that the entire pi_desc struct resides within a
+ * single page, which will be true because the struct is 64 bytes and 64-byte
+ * aligned.
+ */
 struct pi_desc {
 	u32 pir[8];     /* Posted interrupt requested */
 	union {
@@ -6633,6 +6640,14 @@ static void vmx_vcpu_setup(struct vcpu_vmx *vmx)
 	}
 
 	if (kvm_vcpu_apicv_active(&vmx->vcpu)) {
+		/*
+		 * Note, pi_desc is contained within a single
+		 * page because the struct is 64 bytes and 64-byte aligned.
+		 */
+		phys_addr_t pi_desc_phys =
+			page_to_phys(vmalloc_to_page(&vmx->pi_desc)) +
+			(u64)&vmx->pi_desc % PAGE_SIZE;
+
 		vmcs_write64(EOI_EXIT_BITMAP0, 0);
 		vmcs_write64(EOI_EXIT_BITMAP1, 0);
 		vmcs_write64(EOI_EXIT_BITMAP2, 0);
@@ -6641,7 +6656,7 @@ static void vmx_vcpu_setup(struct vcpu_vmx *vmx)
 		vmcs_write16(GUEST_INTR_STATUS, 0);
 
 		vmcs_write16(POSTED_INTR_NV, POSTED_INTR_VECTOR);
-		vmcs_write64(POSTED_INTR_DESC_ADDR, __pa((&vmx->pi_desc)));
+		vmcs_write64(POSTED_INTR_DESC_ADDR, pi_desc_phys);
 	}
 
 	if (!kvm_pause_in_guest(vmx->vcpu.kvm)) {
@@ -11493,13 +11508,18 @@ static void vmx_free_vcpu(struct kvm_vcpu *vcpu)
 	free_loaded_vmcs(vmx->loaded_vmcs);
 	kfree(vmx->guest_msrs);
 	kvm_vcpu_uninit(vcpu);
-	kmem_cache_free(kvm_vcpu_cache, vmx);
+	kmem_cache_free(vmx_msr_entry_cache, vmx->msr_autoload.guest.val);
+	kmem_cache_free(vmx_msr_entry_cache, vmx->msr_autoload.host.val);
+	vfree(vmx);
 }
 
 static struct kvm_vcpu *vmx_create_vcpu(struct kvm *kvm, unsigned int id)
 {
 	int err;
-	struct vcpu_vmx *vmx = kmem_cache_zalloc(kvm_vcpu_cache, GFP_KERNEL);
+	struct vcpu_vmx *vmx =
+		__vmalloc(sizeof(struct vcpu_vmx),
+			  GFP_KERNEL | __GFP_ZERO | __GFP_ACCOUNT,
+			  PAGE_KERNEL);
 	unsigned long *msr_bitmap;
 	int cpu;
 
@@ -11610,7 +11630,7 @@ static struct kvm_vcpu *vmx_create_vcpu(struct kvm *kvm, unsigned int id)
 free_msr_autoload_guest:
 	kmem_cache_free(vmx_msr_entry_cache, vmx->msr_autoload.guest.val);
 free_vmx:
-	kmem_cache_free(kvm_vcpu_cache, vmx);
+	vfree(vmx);
 	return ERR_PTR(err);
 }
 
@@ -15221,8 +15241,11 @@ static int __init vmx_init(void)
 	}
 #endif
 
-	r = kvm_init(&vmx_x86_ops, sizeof(struct vcpu_vmx),
-		__alignof__(struct vcpu_vmx), THIS_MODULE);
+	/*
+	 * Disable kmem cache; vmalloc will be used instead
+	 * to avoid OOM'ing when memory is available but not contiguous.
+	 */
+	r = kvm_init(&vmx_x86_ops, 0, 0, THIS_MODULE);
 	if (r)
 		return r;
 	/*
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 786ade1843a2..8b979e7c3ecd 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -4038,18 +4038,22 @@ int kvm_init(void *opaque, unsigned vcpu_size, unsigned vcpu_align,
 		goto out_free_2;
 	register_reboot_notifier(&kvm_reboot_notifier);
 
-	/* A kmem cache lets us meet the alignment requirements of fx_save. */
-	if (!vcpu_align)
-		vcpu_align = __alignof__(struct kvm_vcpu);
-	kvm_vcpu_cache =
-		kmem_cache_create_usercopy("kvm_vcpu", vcpu_size, vcpu_align,
-					   SLAB_ACCOUNT,
-					   offsetof(struct kvm_vcpu, arch),
-					   sizeof_field(struct kvm_vcpu, arch),
-					   NULL);
-	if (!kvm_vcpu_cache) {
-		r = -ENOMEM;
-		goto out_free_3;
+	/*
+	 * When vcpu_size is zero,
+	 * architecture-specific code manages its own vcpu allocation.
+	 */
+	kvm_vcpu_cache = NULL;
+	if (vcpu_size) {
+		if (!vcpu_align)
+			vcpu_align = __alignof__(struct kvm_vcpu);
+		kvm_vcpu_cache = kmem_cache_create_usercopy(
+			"kvm_vcpu", vcpu_size, vcpu_align, SLAB_ACCOUNT,
+			offsetof(struct kvm_vcpu, arch),
+			sizeof_field(struct kvm_vcpu, arch), NULL);
+		if (!kvm_vcpu_cache) {
+			r = -ENOMEM;
+			goto out_free_3;
+		}
 	}
 
 	r = kvm_async_pf_init();
